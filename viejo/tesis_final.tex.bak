\documentclass[letterpaper,spanish,11pt,oneside]{book}
\usepackage[latin1]{inputenc}
\usepackage[spanish]{babel}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{titlesec}
\usepackage{graphicx}
\usepackage[usenames,dvipsnames]{color}
\usepackage{yfonts}
\usepackage{fancybox}
\newcommand{\bigrule}{\titlerule[0.3mm]}
\titleformat{\chapter}[display]
{\bfseries\Huge}
{
 \titlerule
 \filleft
 \Large\chaptertitlename\
 \Large\thechapter}
{0mm}
{\filleft}
[\vspace{0.5mm} \bigrule]

\linespread{1.14}
\addtolength{\hoffset}{-1cm}
\addtolength{\textwidth}{2cm}
\addtolength{\voffset}{-1cm}
\addtolength{\textheight}{1cm}

\theoremstyle{plain}
\newtheorem{theorem}{Teorema}[section]
\newtheorem{proposition}[theorem]{Proposici\'on}
\newtheorem{lemma}[theorem]{Lema}
\newtheorem{corollary}[theorem]{Corolario}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definici\'on}
\newtheorem{example}[theorem]{Ejemplo}
\newtheorem{examples}[theorem]{Ejemplos}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Observaci\'on}

\title{%{\Large Anteproyecto de Grado}\\
\pagecolor{Aquamarine}
\vspace{-1.5cm}
\includegraphics[width=10cm,height=2.5cm]{./ps/logo3}
\vspace{1.0cm}
\begin{center}
{\huge \bf  DISEÑOS \'OPTIMOS EN LA PRESENCIA DE EFECTOS DE BLOQUES ALEATORIOS}
\end{center}
\vspace{1.0cm}
\author{\textbf{Presentado por:}\\
\sc EDDIE EDINSON RODRIGUEZ BOSSIO\\ \it C\'odigo: 72231783 \\
\vspace{1.0cm}\\
\textbf{Director:}\\
 Dr. rer. nat. \sc Jesús Alonso Cabrera\\
\vspace{1.0cm}}
\date{\textbf{Universidad del Atl\'antico}\\
Facultad de Ciencias B\'asicas\\
Programa de Maestr\'ia en Ciencias Matem\'aticas\\
Barranquilla (Atl\'antico)\\
Septiembre de 2015}}
%--------------------------------------------------------------
\begin{document}
\maketitle
\pagecolor{White}
%\include{portada1}
\include{portad2}
%\include{portada3}
\include{jurado}
\tableofcontents


%--------------------------------------------------------------
\frontmatter
%--------------------------------------------------------------
%
% A partir del comando \frontmatter comienza la unidad frontal
% del libro, i.e. la que contiene prï¿½logos argadecimientos,
% dedicatorias, el indice general etc.
%
%--------------------------------------------------------------




\chapter{Introducci\'on} 
Al interior de los experimentos estad\'isticos la teor\'ia de los diseños \'optimos ha sido desarrollada. En general el tema de esta teor\'ia es que para un apropiado modelo, si queremos poner \'enfasis sobre una cualidad particular de los par\'ametros a estimar, entonces la configuraci\'on experimental deber\'ia ser elegida de acuerdo a ciertos criterios con sentido estad\'istico. En la literatura relacionada con los diseños \'optimos, un prominente autor fue Kiefer (1959), el cu\'al present\'o los principales conceptos, tales como diseños aproximados y una variedad de criterios de \'optimalidad para esta rama de los diseños de experimentos; Kiefer, en particular dio el nombre $ D- $optimalidad al criterio introducido por Wald (1943), este criterio es el m\'as comunmente aplicado y est\'a definido en funci\'on del determinante de la matriz de covarianza.\\

M\'as recientemente son reconocidos los libros de Atkinson y Donev (1992) y Pukelsheim (1993), donde los autores hacen una presentaci\'on estad\'istica formal de los diseños \'optimos. El presente trabajo se ha organizado en tres cap\'itulos: el cap\'itulo uno (Preliminares) contiene conceptos generales que sirven de apoyo y base a la teor\'ia que se desarrolla en los siguientes dos cap\'itulos. El cap\'itulo dos trata sobre diseños \'optimos en la presencia de efectos de bloques aleatorios, y en el cap\'itulo tres se daran a conocer las conclusiones y una serie de problemas abiertos
para futuras investigaciones relacionadas con el tema central de este trabajo de investigaci\'on.\\




\chapter*{Agradecimientos}
En primer lugar doy gracias a Dios dador de la vida y oportunidades; quien puso en mi camino a personas e instituciones que fueron piezas claves para este logro en mi vida. Ellos son mi familia, mi tutor de tesis Dr. rer. nat. Jesús Alonso Cabrera por compartir sus conocimientos y consejos, Dr. Jorge Rodriguez, Dr. Alejandro Urieles, profesores de la maestr\'ia, compañeros de estudio y las instituciones de la Universidad del Atl\'antico donde curs\'e mi pregrado y maestr\'ia, y la Universidad del Norte donde realic\'e mi especializaci\'on en matem\'aticas y se llevaron a cabo las asesor\'ias de este trabajo de grado, ya que mi asesor es docente de planta de esta alma mater.
%--------------------------------------------------------------

%--------------------------------------------------------------
%
% A partir del comando \mainmatter comienza la parte principal
% del libro, i.e. el libro propiamente dicho y los apï¿½ndices
%
%--------------------------------------------------------------
\mainmatter

\chapter{Preliminares}
En el \'area de diseños de experimentos existen, principalmente, dos enfoques competitivos: uno basado en el an\'alisis combinatorio m\'as ajustado para modelos estad\'isticos de an\'alisis de varianza y el otro enfoque basado en m\'etodos an\'aliticos que envuelve el an\'alisis convexo, el cual se aplica, por ejemplo, a superficies de respuesta. Una concisa introducci\'on a este \'ultimo enfoque es dado en la breve monograf\'ia por Silvey (1980).\\

Mientras para modelos en la presencia de efectos fijos se han encontrado diseños \'optimos para una gran variedad de casos (Schwabe, 2003) si los efectos de bloques se asumen que provienen de procesos aleatorios, aparecen dificultades adicionales. Un primer simple ejemplo de polinomios de regresi\'on en la presencia de efectos de factores aleatorios discretos ha sido considerado en los pioneros art\'iculos de Cheng (1995) y Atkins y Cheng (1999). Goos (2000) extendi\'o este resultado a varias estructuras de bloque y obtuvo soluciones num\'ericas. A continuaci\'on presentaremos los conceptos b\'asicos de la teor\'ia de diseños de experimentos.\\


\section{La ecuaci\'on de regresi\'on}
La siguiente ecuaci\'on es b\'asica en la teor\'a de regresi\'on:\\
\begin{equation}
	\mathrm{Y}_{j}=\eta(\mathrm{x}_{j},\boldsymbol{\theta})+\epsilon_{j},  \  j=1,...,N,\label{n=1}
\end{equation}
\\
donde $\mathrm{Y}_{1},...,\mathrm{Y}_{N}$ son resultados experimentales, $\eta(\mathrm{x},\boldsymbol{\theta})$ es una funci\'on dada con vector de par\'ametros desconocidos $\boldsymbol{\theta}=(\theta_{1},...,\theta_{m})^{\top}$, $\epsilon_{1},...,\epsilon_{N}$ corresponden al error de observaci\'on y $\mathrm{x}_{1},...,\mathrm{x}_{N}$ son condiciones experimentales, que pertenecen a un conjunto compacto $\mathcal{X}$ usualmente llamado la regi\'on de diseño.\\

Los casos para representar los resultados de los experimentos reales en la forma (1.1) se ha demostrado en muchos ejemplos, ver Rao (1973), Federow (1972), y Pukelsheim (1993).\\

Recordemos algunos supuestos básicos del modelo clásico de regresi\'on.
\begin{itemize}
	\item[(a)] Insesgamiento: $\mathrm{E}({\epsilon_{j}})=0$; $(j=1,...,N)$. Esto significa que $\mathrm{E}({\mathrm{Y}_{j}})=\eta(\mathrm{x}_{j},\boldsymbol{\theta})$ (es decir, el modelo est\'a libre de un error sistem\'atico).
	\item[(b)] Incorrelaci\'on: $\mathrm{E}({\epsilon_{i}\epsilon_{j}})=0$; $(i\neq j)$.
	\item[(c)] Homogeneidad de varianza: $\mathrm{E}({\epsilon_{j}^{2}})\equiv\sigma^{2}>0$; $(j=1,...,N).$
	\item[(d)] La linealidad de parametrización: $\eta(\mathrm{x},\boldsymbol{\theta})=\boldsymbol{f}(\mathrm{x})^{\top}\boldsymbol{\theta}$, donde $\boldsymbol{f}(\mathrm{x})=(f_{1}(\mathrm{x}),...,f_{m}(\mathrm{x}))^{\top}$, $f_{i}(\mathrm{x})$, $i=1,...,m$, son funciones b\'asicas y conocidas.
\end{itemize}

Como es habitual en la teor\'ia estad\'istica, estos supuestos proporcionan resultados observables a obtener y corresponden en cierta medida a las características de experimentos reales.\\

El prop\'osito principal de un experimento es estimar un vector de par\'ametros desconocidos, o probar una hip\'otesis sobre los valores de los par\'ametros. Aqu\'i, la exactitud de conclusiones estadísticas depende tanto del m\'etodo de la inferencia estad\'istica y en la elecci\'on de las condiciones experimentales.\\

Si $(a)$ - $(b)$ se asumen, entonces la t\'ecnica de m\'inimos cuadrados, proporciona valoraci\'on del vector $\boldsymbol{\theta}$ de par\'ametros bajo cualesquiera condiciones experimentales fijas.\\

\section{Estimaci\'on de m\'inimos cuadrados}
La paternidad de este m\'etodo se reparte entre Legendre que lo public\'o en 1805 y Gauss que lo utiliz\'o en 1795 y lo public\'o en 1809.\\

El m\'etodo de m\'inmos cuadrados $(MC)$ es utilizado para estimar los par\'ametros en el modelo de regresi\'on lineal.\\

Por ejemplo, en el modelo de regresi\'on lineal m\'ultiple\\
\begin{equation}
\begin{aligned}
	\nonumber
\mathrm{Y}_{j} & = \beta_{0}+\beta_{1}\mathrm{x}_{j1}+\cdots+\beta_{k}\mathrm{x}_{jk}+\epsilon_{j} \label{n=1} \\\\
	\nonumber                & = \beta_{0}+\sum_{i=1}^k\beta_{i}\mathrm{x}_{ji}+\epsilon_{j} \ ,j=1,...,N.
\end{aligned}
\end{equation}
\\
Suponga que se tiene $N>k$ observaciones. Se asume que $\mathrm{E}({\epsilon_{j}})=0$ y $Var(\epsilon_{j})=\sigma^{2}$ y que los errores son independientes. El m\'etodo de m\'inimos cuadrados m\'inimiza la suma de cuadrados del error dada por\\
\begin{equation}
\begin{aligned}
\nonumber
SSE & = \sum_{J=1}^N\epsilon_{j}^{2} \\\\
\nonumber
& = \sum_{J=1}^N \left(\mathrm{Y}_{j}-\beta_{0}-\sum_{i=1}^k\beta_{i}\mathrm{x}_{ji}\right)^{2} \\
\end{aligned}
\end{equation}
\\
con respecto a cada uno de los par\'ametros del modelo $\beta_{0},\beta_{1},...,\beta_{k}$.\\

La derivada con respecto a $\beta_{0}$\\
\begin{equation}
\begin{aligned}
\nonumber
\frac{\partial SSE}{\partial \beta_{0}} & = \frac{\partial}{\partial \beta_{0}}\sum_{J=1}^N \left(\mathrm{Y}_{j}-\beta_{0}-\sum_{i=1}^k\beta_{i}\mathrm{x}_{ji}\right)^{2} \\\\
\nonumber
& = -2\sum_{J=1}^N \left(\mathrm{Y}_{j}-\beta_{0}-\sum_{i=1}^k\beta_{i}\mathrm{x}_{ji}\right) \\
\end{aligned}
\end{equation}
\\

La derivada con respecto a $\beta_{j}  \  (j=1,2,...,k)$ es
\begin{equation}
\begin{aligned}
\nonumber
\frac{\partial SSE}{\partial \beta_{j}} & = \frac{\partial}{\partial \beta_{j}}\sum_{J=1}^N \left(\mathrm{Y}_{j}-\beta_{0}-\sum_{i=1}^k\beta_{i}\mathrm{x}_{ji}\right)^{2} \\\\
\nonumber
& = -2\sum_{J=1}^N \left(\mathrm{Y}_{j}-\beta_{0}-\sum_{i=1}^k\beta_{i}\mathrm{x}_{ji}\right)\mathrm{x}_{ji} \\
\end{aligned}
\end{equation}
\\

igualando a cero  las derivadas, se tiene\\

$$\frac{\partial SSE}{\partial \beta_{0}} = -2\sum_{J=1}^N \left(\mathrm{Y}_{j}-\beta_{0}-\sum_{i=1}^k\beta_{i}\mathrm{x}_{ji}\right)=0$$
\\
$$\frac{\partial SSE}{\partial \beta_{j}} =  -2\sum_{J=1}^N \left(\mathrm{Y}_{j}-\beta_{0}-\sum_{i=1}^k\beta_{i}\mathrm{x}_{ji}\right)\mathrm{x}_{ji}=0$$ 
\\\\

Simplificando para $\beta_{0}$ se tiene\\
\begin{equation}
\begin{aligned}
\nonumber
\sum_{j=1}^N\mathrm{Y}_{j}-\sum_{j=1}^N\beta_{0}-\sum_{j=1}^N\sum_{i=1}^k\beta_{i}\mathrm{x}_{ji} & = 0\\\\
\sum_{j=1}^N\beta_{0}+\sum_{j=1}^N\sum_{i=1}^k\beta_{i}\mathrm{x}_{ji} & = \sum_{j=1}^N\mathrm{Y}_{j} \\\\
N\beta_{0}+\sum_{j=1}^N\sum_{i=1}^k\beta_{i}\mathrm{x}_{ji} & = \sum_{j=1}^N\mathrm{Y}_{j} \\\\
N\beta_{0}+\sum_{j=1}^N\left(\beta_{1}\mathrm{x}_{j1}+\beta_{2}\mathrm{x}_{j2}+\cdots+\beta_{k}\mathrm{x}_{jk}\right) & = \sum_{j=1}^N\mathrm{Y}_{j} \\\\
N\beta_{0}+\beta_{1}\sum_{j=1}^N\mathrm{x}_{j1}+\beta_{2}\sum_{j=1}^N\mathrm{x}_{j2}+\cdots+\beta_{k}\sum_{j=1}^N\mathrm{x}_{jk} & = \sum_{j=1}^N\mathrm{Y}_{j} \\
\end{aligned}
\end{equation}
\\\\

Simplificando para $\beta_{j}$ se tiene\\
\begin{equation}
\begin{aligned}
\nonumber
\sum_{J=1}^N \left(\mathrm{Y}_{j}-\beta_{0}-\sum_{i=1}^k\beta_{i}\mathrm{x}_{ji}\right)\mathrm{x}_{ji} & = 0\\\\
\left(\sum_{J=1}^N \mathrm{Y}_{j}-\sum_{J=1}^N\beta_{0}-\sum_{J=1}^N\sum_{i=1}^k\beta_{i}\mathrm{x}_{ji}\right)\mathrm{x}_{ji} & = 0 \\\\
\sum_{J=1}^N \mathrm{Y}_{j}\mathrm{x}_{ji}-\beta_{0}\sum_{J=1}^N\mathrm{x}_{ji}-\left(\sum_{J=1}^N\sum_{i=1}^k\beta_{i}\mathrm{x}_{ji}\right)\mathrm{x}_{ji} & = 0 \\\\
\sum_{J=1}^N \mathrm{Y}_{j}\mathrm{x}_{ji}-\beta_{0}\sum_{J=1}^N\mathrm{x}_{ji}-\sum_{J=1}^N\left(\beta_{1}\mathrm{x}_{j1}+\cdots+\beta_{k}\mathrm{x}_{jk}\right)\mathrm{x}_{ji} & = 0 \\\\
\sum_{J=1}^N \mathrm{Y}_{j}\mathrm{x}_{ji}-\beta_{0}\sum_{J=1}^N\mathrm{x}_{ji}-\left(\sum_{J=1}^N\beta_{1}\mathrm{x}_{j1}+\cdots+\beta_{k}\sum_{J=1}^N\mathrm{x}_{jk}\right)\mathrm{x}_{ji} & = 0 \\\\
\beta_{0}\sum_{J=1}^N\mathrm{x}_{ji}+\left(\sum_{J=1}^N\beta_{1}\mathrm{x}_{j1}+\cdots+\beta_{k}\sum_{J=1}^N\mathrm{x}_{jk}\right)\mathrm{x}_{ji} & = \sum_{J=1}^N \mathrm{Y}_{j}\mathrm{x}_{ji} \\
\end{aligned}
\end{equation}
\\\\



Luego las ecuaciones normales son:
\begin{equation}
\begin{aligned}
\nonumber
N\beta_{0}+\beta_{1}\sum_{j=1}^N\mathrm{x}_{j1}+\cdots+\beta_{k}\sum_{j=1}^N\mathrm{x}_{jk} & = \sum_{j=1}^N\mathrm{Y}_{j} \\\\
\beta_{0}\sum_{J=1}^N\mathrm{x}_{j1}+\beta_{1}\sum_{J=1}^N\mathrm{x}_{j1}^{2}+\cdots+\beta_{k}\sum_{J=1}^N\mathrm{x}_{jk}\mathrm{x}_{j1} & = \sum_{J=1}^N \mathrm{Y}_{j}\mathrm{x}_{j1} \\\\
\beta_{0}\sum_{J=1}^N\mathrm{x}_{j2}+\beta_{1}\sum_{J=1}^N\mathrm{x}_{j1}\mathrm{x}_{j2}+\cdots+\beta_{k}\sum_{J=1}^N\mathrm{x}_{jk}\mathrm{x}_{j2} & = \sum_{J=1}^N \mathrm{Y}_{j}\mathrm{x}_{j2}  \\
 & \vdots \\
 \beta_{0}\sum_{J=1}^N\mathrm{x}_{jk}+\beta_{1}\sum_{J=1}^N\mathrm{x}_{j1}\mathrm{x}_{jk}+\cdots+\beta_{k}\sum_{J=1}^N\mathrm{x}_{jk}^{2} & = \sum_{J=1}^N \mathrm{Y}_{j}\mathrm{x}_{jk} \\
\end{aligned}
\end{equation}
\\

Observe que hay $p=k+1$ ecuaciones. Para obtener la soluci\'on es conveniente utilizar notaci\'on matricial. En esta notaci\'on el modelo se expresa como\\

$$\mathrm{Y}=\mathrm{X}\boldsymbol{\beta}
+\epsilon$$
\\
donde\\
$\mathrm{Y}$ es el vector de observaciones\\
$\mathrm{X}$ es una matriz $n$ x $p$ de niveles de la variable\\
$\boldsymbol{\beta}$ es un vector $p$ x $1$ de coeficientes de regresi\'on\\
$\epsilon$ es el vector aleatorio error de orden $p$ x $1$\\
\\

La suma de cuadrados del error es dada por\\ 
$$SSE = \sum_{J=1}^N\epsilon_{j}^{2}=\epsilon'\epsilon=(\mathrm{Y}-\mathrm{X}\boldsymbol{\beta})'(\mathrm{Y}-\mathrm{X}\boldsymbol{\beta})$$
\\

Luego se obtiene que las ecuaciones normales son\\
$$\mathrm{X}'\mathrm{X}\boldsymbol{\widehat{\beta}}=\mathrm{X}'\mathrm{Y}$$
\\

Para solucionar las ecuaciones normales se requiere que exista la inversa de la matriz $\mathrm{X}'\mathrm{X}$. Esta existe siempre que las variables regresoras sean linealmente independientes. As\'i, la soluci\'on de m\'inimos cuadrados de vector par\'ametrico $\boldsymbol{\beta}$ es\\
$$\boldsymbol{\widehat{\beta}}=(\mathrm{X}^{\top}\mathrm{X})^{-1}\mathrm{X}^{\top}\mathrm{Y}$$
\\

\section*{Ejemplo 1}
Myers y Montgomery (1985) describen un experimento que trata con motores de turbina de gas. El voltaje de salida de los motores se midi\'o en diversas combinaciones de velocidad de la cuchilla y extensión del sensor de medici\'on de tensi\'on. Los datos del experimento se dan en el Cuadro 1.1. \\
\begin{table}[htbp]
	\centering
	\begin{tabular}{|c|c|c|c|}
		\hline
		\bf $\mathrm{Corrida}$ & 
		\bf $\mathrm{Voltaje}$ & 
		\bf $\mathrm{Velocidad \ de \ la \ cuchilla \ (pulg/seg)}$ & 
		\bf $\mathrm{Extension \ (pulg)}$ \\
		\hline
		1 & 1.23 & 5300 & 0.000\\
		\hline
		2 & 3.13 & 8300 & 0.000\\
		\hline
		3 & 1.22 & 5300 & 0.012\\
		\hline
		4 & 1.92 & 8300 & 0.012\\
		\hline
		5 & 2.02 & 6800 & 0.000\\
		\hline
		6 & 1.51 & 6800 & 0.012\\
		\hline
		7 & 1.32 & 5300 & 0.006\\
		\hline
		8 & 2.62 & 8300 & 0.006\\
		\hline
		9 & 1.65 & 6800 & 0.006\\
		\hline
		10 & 1.62 & 6800 & 0.006\\
		\hline
		11 & 1.59 & 6800 & 0.006\\
		\hline
	\end{tabular}
	\caption{\small Experimento turbina de gas descrito por Myers y Montgomery (1995).} \label{tabJupiter}
\end{table} 
\\

El prop\'osito del experimento fue estimar un modelo cuadr\'atico completo con el voltaje de salida como la variable dependiente y la velocidad de la cuchilla y extensi\'on del sensor de medici\'on de tensi\'on como las variables explicativas.\\


Variables codificadas
\\

Es conveniente para la mayor\'ia de aplicaciones describir el experimento en t\'erminos de variables codificadas, porque esto facilita la comparaci\'on de los diseños de diferentes experimentos. Por lo tanto, se reajustar\'an las variables cuantitativas. Es caracter\'istico de una variable $u$ cuantitativa o continua que var\'ia entre un valor m\'inimo y m\'aximo, $u_{min}$ y $u_{max}$. Por lo general, los niveles de los factores se reajustar\'an a estar entre $-1$ y $+1$. Los valores codificados pueden ser calculados por\\
$$
z=\frac{u-u_{0}}{\Delta}
$$
\\
donde $u_{0}$ es el punto medio del intervalo $[u_{min},u_{max}]$ y $\Delta$ es la mitad de la diferencia entre $u_{max}$ y $u_{min}$. Para la interpretaci\'on de los resultados experimentales, sin embargo, es deseable para volver a los niveles de los factores originales. Para el experimento de turbina de gas, los niveles codificados se pueden obtener de la siguiente manera:\\

$$
\mathrm{x}_{1}=\frac{Velocidad de la cuchilla-6800}{1500}
$$
\\
y
\\
$$
\mathrm{x}_{2}=\frac{Extension-0.006}{0.006}
$$
\\

donde $\mathrm{x}_{1}$ y $\mathrm{x}_{2}$ representan los niveles codificados de los factores de velocidad de la cuchilla y extensión del sensor de medición de tensi\'on, respectivamente. Los niveles codificados se muestran en el Cuadro 1.2. Vamos a utilizar esta forma de analizar los datos. Del Cuadro 1.2, es fácil ver que las corridas $9$, $10$ y $11$ se llevan a cabo en el nivel medio de los factores experimentales. \\

\begin{table}[htbp]
	\centering
	\begin{tabular}{|c|c|c|c|}
		\hline
		\bf $\mathrm{Corrida}$ & 
		\bf $\mathrm{Voltaje}$ & 
		\bf $\mathrm{Velocidad \ de \ la \ cuchilla \ (pulg/seg)}$ & 
		\bf $\mathrm{Extension \ (pulg)}$ \\
		\hline
		1 & 1.23 & -1 & -1\\
		\hline
		2 & 3.13 & +1 & -1\\
		\hline
		3 & 1.22 & -1 & +1\\
		\hline
		4 & 1.92 & +1 & +1\\
		\hline
		5 & 2.02 & 0 & -1\\
		\hline
		6 & 1.51 & 0 & +1\\
		\hline
		7 & 1.32 & -1 & 0\\
		\hline
		8 & 2.62 & +1 & 0\\
		\hline
		9 & 1.65 & 0 & 0\\
		\hline
		10 & 1.62 & 0 & 0\\
		\hline
		11 & 1.59 & 0 & 0\\
		\hline
	\end{tabular}
	\caption{\small Forma codificada del experimento de turbinas de gas.} \label{tabJupiter}
	\end{table} 
	
An\'alisis
\\

El prop\'osito del experimento fue estimar un modelo cuadr\'atico completo en las dos variables. Como resultado, la expansi\'on polin\'omica
\\
$$
\boldsymbol{f}^{\top}(\mathrm{x})=\left[
\begin{array}{rrrrrr}
1 & \mathrm{x}_{1} &  \mathrm{x}_{2} & \mathrm{x}_{1}\mathrm{x}_{2} &  \mathrm{x}_{1}^{2} & \mathrm{x}_{2}^{2}
\end{array}
\right]
$$

y

$$
\boldsymbol{\beta}^{\top}=\left[
\begin{array}{rrrrrr}
\beta_{0} & \beta_{1} &  \beta_{2} & \beta_{12} &  \beta_{11} & \beta_{22}
\end{array}
\right],
$$
\\
de manera que $p=6$ y el modelo estad\'istico se pueden escribir como
\\
$$\mathrm{Y}=\beta_{0}+\beta_{1}\mathrm{x}_{1}+\beta_{2}\mathrm{x}_{2}+\beta_{12}\mathrm{x}_{1}\mathrm{x}_{2}+\beta_{11}\mathrm{x}_{1}^{2}+\beta_{22}\mathrm{x}_{2}^{2}+\epsilon.$$
\\

La matriz de diseño de todo el experimento viene dada por

$$
X=
\left(
\begin{array}{rrrrrr}
	1 & -1 & -1 & +1 & +1 & +1 \\
	1 & +1 & -1 & -1 & +1 & +1 \\
	1 & -1 & +1 & -1 & +1 & +1 \\
	1 & +1 & +1 & +1 & +1 & +1 \\
	1 & 0 & -1 & 0 & 0 & +1 \\
	1 & 0 & +1 & 0 & 0 & +1 \\
	1 & -1 & 0 & 0 & +1 & 0 \\
	1 & +1 & 0 & 0 & +1 & 0 \\
	1 & 0 & 0 & 0 & 0 & 0 \\
	1 & 0 & 0 & 0 & 0 & 0 \\
	1 & 0 & 0 & 0 & 0 & 0 
\end{array}
\right)
$$
\\

la matriz $\mathrm{X}^{\top}\mathrm{X}$ es\\
$$
\left(
\begin{array}{rrrrrr}
11 & 0 & 0 & 0 & 6 & 6 \\
0 & 6 & 0 & 0 & 0 & 0 \\
0 & 0 & 6 & 0 & 0 & 0 \\
0 & 0 & 0 & 4 & 0 & 0 \\
6 & 0 & 0 & 0 & 6 & 4 \\
6 & 0 & 0 & 0 & 4 & 6
\end{array}
\right)
$$
\\

y entonces $(\mathrm{X}^{\top}\mathrm{X})^{-1}$ es\\

$$
\left(
\begin{array}{rrrrrr}
0.2632 & 0 & 0 & 0 & -0.1579 & -0.1579 \\
0 & 0.1667 & 0 & 0 & 0 & 0 \\
0 & 0 & 0.1667 & 0 & 0 & 0 \\
0 & 0 & 0 & 0.25 & 0 & 0 \\
-0.1579 & 0 & 0 & 0 & 0.3947 & -0.1053 \\
-0.1579 & 0 & 0 & 0 & -0.1053 & 0.3947
\end{array}
\right)
$$
\\\\


El vector $\mathrm{Y}$ es\\
$$
Y=\left(
\begin{array}{rrr}
1.23 \\
3.13 \\
1.22 \\
1.92 \\
2.02 \\
1.51 \\
1.32 \\
2.62 \\
1.65 \\
1.62 \\
1.59
\end{array}
\right) \
$$
\\

y el vector $\mathrm{X}^{\top}\mathrm{Y}$ es\\
$$
\left(
\begin{array}{rrr}
19.83 \\
3.9 \\
-1.73 \\
-1.2 \\
11.44 \\
11.03 
\end{array}
\right)
$$
\\\\

el estimador de m\'inimos cuadrados de $\boldsymbol{\beta}$ es
\\

$$\boldsymbol{\widehat{\beta}}=(\mathrm{X}^{\top}\mathrm{X})^{-1}\mathrm{X}^{\top}\mathrm{Y}$$
\

o
\

\begin{equation}
\begin{aligned}
\nonumber
\boldsymbol{\widehat{\beta}} & = \left(
\begin{array}{rrr}
\widehat{\beta}_{0} \\
\widehat{\beta}_{1} \\
\widehat{\beta}_{2} \\
\widehat{\beta}_{12} \\
\widehat{\beta}_{11} \\
\widehat{\beta}_{22}
\end{array}
\right) \label{n=1} \\\\
\nonumber                & = \left(
\begin{array}{rrrrrr}
0.2632 & 0 & 0 & 0 & -0.1579 & -0.1579 \\
0 & 0.1667 & 0 & 0 & 0 & 0 \\
0 & 0 & 0.1667 & 0 & 0 & 0 \\
0 & 0 & 0 & 0.25 & 0 & 0 \\
-0.1579 & 0 & 0 & 0 & 0.3947 & -0.1053 \\
-0.1579 & 0 & 0 & 0 & -0.1053 & 0.3947
\end{array}
\right)
\left(
\begin{array}{rrr}
19.83 \\
3.9 \\
-1.73 \\
-1.2 \\
11.44 \\
11.03 
\end{array}
\right)\\\\
\nonumber    & =\left(
\begin{array}{rrr}
1.6705 \\
0.6500\\
-0.2883 \\
-0.3000 \\
0.2237 \\
0.0187
\end{array}
\right)\\\\
\nonumber
\end{aligned}
\end{equation}


luego el modelo ajustado por m\'inimos cuadrados es\\
\begin{equation}
\begin{aligned}
\nonumber
\widehat{\mathrm{Y}} & = 1.6706+0.6500\mathrm{x}_{1}-0.2883\mathrm{x}_{2}-0.3000\mathrm{x}_{1}\mathrm{x}_{2}+0.2237\mathrm{x}_{1}^{2}+0.0187\mathrm{x}_{2}^{2}
\end{aligned}
\end{equation}
\\

\section{Propiedades de los estimadores de m\'inimos cuadrados}

\begin{itemize}
	\item[(1)]$\boldsymbol{\widehat{\beta}}$ es un estimador insesgado de $\boldsymbol{\beta}$. \'Esto es, $\mathrm{E}(\boldsymbol{\widehat{\beta}})=\boldsymbol{\beta}$.\\
	
	 Demostraci\'on:\\
\begin{equation}
	\begin{aligned}
		\nonumber
		\mathrm{E}({\boldsymbol{\widehat{\beta}}})&=\mathrm{E}[(\mathrm{X}^{\top}\mathrm{X})^{-1}\mathrm{X}^{\top}\mathrm{Y}]  \\
		\nonumber
		& = (\mathrm{X}^{\top}\mathrm{X})^{-1}\mathrm{X}^{\top}\mathrm{E}(\mathrm{Y}) \\
	\end{aligned}
\end{equation}

y como $\mathrm{E}(\mathrm{Y})=\mathrm{X}\boldsymbol{\widehat{\beta}}$, entonces\\
\begin{equation}
\begin{aligned}
\nonumber
\mathrm{E}({\boldsymbol{\widehat{\beta}}})&=(\mathrm{X}^{\top}\mathrm{X})^{-1}\mathrm{X}^{\top}\mathrm{X}\boldsymbol{\beta}  \\
\nonumber
& = \boldsymbol{\beta} \\
\end{aligned}
\end{equation}

\item[(2)] La matriz de varianzas y covarianzas del vector $\boldsymbol{\widehat{\beta}}$ es $Cov(\boldsymbol{\widehat{\beta}})=Var(\boldsymbol{\widehat{\beta}})=\sigma^{2}(\mathrm{X}^{\top}\mathrm{X})^{-1}$

Demostraci\'on:\\
$Var(\boldsymbol{\widehat{\beta}})=Var((\mathrm{X}^{\top}\mathrm{X})^{-1}\mathrm{X}^{\top}\mathrm{Y})$\\
Sea $\mathrm{A}=(\mathrm{X}^{\top}\mathrm{X})^{-1}\mathrm{X}^{\top}$ y como $\mathrm{A}$ es una matriz y $\mathrm{Y}$ un vector columna y por la pripiedad de varianzas $Var(\mathrm{A}\mathrm{Y})=\mathrm{A}Var(\mathrm{Y})\mathrm{A}^{\top}$, se tiene que\\
\begin{equation}
\begin{aligned}
\nonumber
Var(\boldsymbol{\widehat{\beta}})&=(\mathrm{X}^{\top}\mathrm{X})^{-1}\mathrm{X}^{\top}Var(\mathrm{Y})((\mathrm{X}^{\top}\mathrm{X})^{-1}\mathrm{X}^{\top})^{\top}  \\
\nonumber
& = (\mathrm{X}^{\top}\mathrm{X})^{-1}\mathrm{X}^{\top}\sigma^{2}((\mathrm{X}^{\top}\mathrm{X})^{-1}\mathrm{X}^{\top})^{\top} \\
\nonumber
& = (\mathrm{X}^{\top}\mathrm{X})^{-1}\mathrm{X}^{\top}\sigma^{2}(\mathrm{X}^{\top})^{\top}((\mathrm{X}^{\top}\mathrm{X})^{-1})^{\top} \\
\nonumber
& = (\mathrm{X}^{\top}\mathrm{X})^{-1}\mathrm{X}^{\top}\sigma^{2}\mathrm{X}(\mathrm{X}^{\top}\mathrm{X})^{-1} \\
\nonumber
& = \sigma^{2}(\mathrm{X}^{\top}\mathrm{X})^{-1}(\mathrm{X}^{\top}\mathrm{X})(\mathrm{X}^{\top}\mathrm{X})^{-1} \\
\nonumber
& = \sigma^{2}(\mathrm{X}^{\top}\mathrm{X})^{-1}\mathrm{I} \\
\nonumber
& = \sigma^{2}(\mathrm{X}^{\top}\mathrm{X})^{-1} \\
\end{aligned}
\end{equation}
\end{itemize}
\section*{Ejemplo 2}
Para los datos del ejemplo 1, se tiene que la estimaci\'on de la matriz de varianzas-covarianzas del vector $\boldsymbol{\widehat{\beta}}$ es\\
\begin{equation}
\begin{aligned}
\nonumber
Var(\boldsymbol{\widehat{\beta}}) & = \sigma^{2}(\mathrm{X}^{\top}\mathrm{X})^{-1} \\\\
\nonumber
& = 0.0083\left(
\begin{array}{rrrrrr}
0.2632 & 0 & 0 & 0 & -0.1579 & -0.1579 \\
0 & 0.1667 & 0 & 0 & 0 & 0 \\
0 & 0 & 0.1667 & 0 & 0 & 0 \\
0 & 0 & 0 & 0.25 & 0 & 0 \\
-0.1579 & 0 & 0 & 0 & 0.3947 & -0.1053 \\
-0.1579 & 0 & 0 & 0 & -0.1053 & 0.3947
\end{array}
\right)
\\\\
\nonumber
& = \left(
\begin{array}{rrrrrr}
0.00218456 & 0 & 0 & 0 & -0.00131057 & -0.00131057 \\
0 & 0.00138361 & 0 & 0 & 0 & 0 \\
0 & 0 & 0.00138361 & 0 & 0 & 0 \\
0 & 0 & 0 & 0.002075 & 0 & 0 \\
-0.00131057 & 0 & 0 & 0 & 0.00327601 & -0.00087399 \\
-0.00131057 & 0 & 0 & 0 & -0.00087399 & 0.00327601
\end{array}
\right)
\\
\end{aligned}
\end{equation}
\\


Los errores est\'andar de cada par\'ametro es dado en la tabla:
\\
$$
\begin{tabular}{||c||c||}
	\hline
	\bf Par\'ametro & \bf Error est\'andar \\
	\hline \hline
	$\beta_{0}$ & $\displaystyle \sqrt{0.00218456}=0.04673928$  \\
	\hline \hline
	$\beta_{1}$ , \ $\beta_{2}$ & $\displaystyle \sqrt{0.00138361}=0.03719691$ \\
	\hline \hline
	$\beta_{12}$ & $\displaystyle \sqrt{0.002075}=0.04555217$ \\
	\hline \hline
	$\beta_{11}$ , $\beta_{22}$ & $\displaystyle \sqrt{0.00327601}=0.05723644$ \\
	\hline
\end{tabular}
$$
\\

\section{Teorema de Gauss - Markov}
Si no se asume normalidad el estimador m\'inimo cuadr\'atico $\boldsymbol{\widehat{\beta}}$ es el mejor estimador dentro de los estimadores lineales insesgados de $\boldsymbol{\beta}$, en el sentido que es el de la varianza m\'as pequeña.\\

Demostraci\'on:\\

Conocemos que $\boldsymbol{\widehat{\beta}}=(\mathrm{X}^{\top}\mathrm{X})^{-1}\mathrm{X}^{\top}\mathrm{Y}$, sea $\mathrm{A}=(\mathrm{X}^{\top}\mathrm{X})^{-1}\mathrm{X}^{\top}$, entonces $\boldsymbol{\widehat{\beta}}=\mathrm{A}\mathrm{Y}$.\\
Demostremos que $\boldsymbol{\beta}=\mathrm{C}\mathrm{Y}$ no es mejor estimador insesgado que $\boldsymbol{\widehat{\beta}}$. Sabemos que $\mathrm{E}[\mathrm{C}\mathrm{Y}]=\mathrm{C}\mathrm{E}[\mathrm{Y}]=\mathrm{C}\mathrm{X}\boldsymbol{\beta}$ por ser $\boldsymbol{\beta}$ insesgado se cumple que $\mathrm{E}[\boldsymbol{\beta}]=\boldsymbol{\beta}$ $\Longrightarrow$ $\mathrm{C}\mathrm{X}\boldsymbol{\beta}=\boldsymbol{\beta}$ $\Longrightarrow$ $\mathrm{C}\mathrm{X}\boldsymbol{\beta}-\boldsymbol{\beta}=\overrightarrow{0}$ $\Longrightarrow$ $(\mathrm{C}\mathrm{X}-\mathrm{I})\boldsymbol{\beta}=\overrightarrow{0}$, como $\boldsymbol{\beta}=0$, entonces $\Longrightarrow$ $\mathrm{C}\mathrm{X}-\mathrm{I}=\overrightarrow{0}$ $\Longrightarrow$ $\mathrm{C}\mathrm{X}=\mathrm{I}$.\\
Ahora como $\mathrm{C}$ es una matriz y $\mathrm{Y}$ un vector columna y por la propiedad $Var(\mathrm{A}\mathrm{Y})=\mathrm{A}Var(\mathrm{Y})\mathrm{A}^{\top}$, tenemos que $Var(\mathrm{C}\mathrm{Y})=\mathrm{C}Var(\mathrm{Y})\mathrm{C}^{\top}=\mathrm{C}\sigma^{2}\mathrm{C}^{\top}=\sigma^{2}\mathrm{C}\mathrm{C}^{\top}$.\\
Luego como $Var(\boldsymbol{\widehat{\beta}})=\sigma^{2}(\mathrm{X}^{\top}\mathrm{X})^{-1}$ y $\mathrm{C}\mathrm{X}=\mathrm{I}$  y $(\mathrm{C}\mathrm{X})\top=\mathrm{X}^{\top}\mathrm{C}^{\top}=\mathrm{I}$, podemos escribir $Var(\boldsymbol{\widehat{\beta}})=\mathrm{C}\mathrm{X}\sigma^{2}(\mathrm{X}^{\top}\mathrm{X})^{-1}\mathrm{X}^{\top}\mathrm{C}^{\top}=\sigma^{2}\mathrm{C}\mathrm{X}(\mathrm{X}^{\top}\mathrm{X})^{-1}\mathrm{X}^{\top}\mathrm{C}^{\top}$\\

por lo tanto\\
\begin{equation}
\begin{aligned}
\nonumber
Var(\boldsymbol{\beta})-Var(\boldsymbol{\widehat{\beta}})&=\sigma^{2}\mathrm{C}\mathrm{C}^{\top}-\sigma^{2}\mathrm{C}\mathrm{X}(\mathrm{X}^{\top}\mathrm{X})^{-1}\mathrm{X}^{\top}\mathrm{C}^{\top}  \\
\nonumber
& = \sigma^{2}\mathrm{C}(\mathrm{I}-\mathrm{X}(\mathrm{X}^{\top}\mathrm{X})^{-1}\mathrm{X}^{\top})\mathrm{C}^{\top} \\
\nonumber
& = \sigma^{2}\mathrm{C}\mathrm{H}\mathrm{C}^{\top} \\
\end{aligned}
\end{equation}
\\

$\mathrm{H}$ es sim\'etrica $(\mathrm{H}^{\top}=\mathrm{H})$ e idempotente $(\mathrm{H}^{2}=\mathrm{H})$, $\mathrm{H}$ es definidida positiva (Una matriz $\mathrm{A}$ se dice que es semidefinida positiva si $\mathrm{Y}^{\top}\mathrm{A}\mathrm{Y}\geq0$ para todo vector $\mathrm{Y}\neq0$. Diremos que es definida positiva si $\mathrm{Y}^{\top}\mathrm{A}\mathrm{Y}>0$ para todo vector $\mathrm{Y}\neq0$), por tanto $\mathrm{C}\mathrm{H}\mathrm{C}^{\top}>0$.\\

Entonces $Var(\boldsymbol{\beta})-Var(\boldsymbol{\widehat{\beta}})>0$ $\Longrightarrow$  $Var(\boldsymbol{\beta})>Var(\boldsymbol{\widehat{\beta}})$. Luego $\boldsymbol{\widehat{\beta}}$ es el mejor estimador insesgado de m\'inima varianza.\\


\section{Diseños experimentales y matrices de informaci\'on}
El conjunto $\{\mathrm{x}_{1},...,\mathrm{x}_{N}\}$ de elementos de $\mathcal{X}$ (aunque algunos de los elementos pueden coincidir unos con otros) se llama un diseño exacto (o discreto) de tamaño $N$.\\

Tengamos s\'olo $n < N$ puntos distintos. Supongamos que $\mathrm{x}_{i}$ ocurre $r_{i}$ veces entre los puntos $\{\mathrm{x}_{1},...,\mathrm{x}_{N}\}$ para $ i=1,...,n$ as\'i 
$$N = \sum_{i=1}^n r_{i}$$. \\

Asociamos $w_{i}=r_{i}/N$ con cada uno de los puntos $\mathrm{x}_{i}$, $i=1,...,n$. Asi \\
\begin{equation}
	\xi =
	\left(
	\begin{array}{lcr}
		x_{1} & ... & x_{n} \\
		w_{1} & ... & w_{n} \\
	\end{array}
	\right)\label{n=2}
\end{equation}
\\
ser\'a llamada diseño exacto (discreto) o diseño $n$-puntos de tamaño $N$.\\

La matriz
\begin{equation}
	M(\xi)=\sum_{i=1}^n f(\mathrm{x}_{i})f^{\top}(\mathrm{x}_{i})w_{i}\label{n=3}
\end{equation}
\\
se llama la matriz de información de diseño $\xi$.\\

Por el teorema de Gauss-Markov, tenemos  
\begin{equation}
	Cov(\boldsymbol{\theta})=\frac{\sigma^{2}}{N}M^{-1}(\xi)\nonumber
\end{equation}
para la matriz varianza de la estimaci\'on de m\'inimos cuadrados.\\

El diseño $\xi$ es una medida de probabilidad discreta, definido por (1.2), que incluye los puntos del conjunto $\mathcal{X}$ y los coeficientes de peso.\\

En muchas situaciones pr\'acticas, es imposible de realizar estos diseños y tales diseños se deben considerar como aproximaci\'on de algunos diseños discretos.\\

Vamos a escribir un diseño, concentrado en un n\'umero finito de puntos, en la forma (1.2), donde los coeficientes $w_{i}=\xi(\mathrm{x}_{i})$ son n\'umeros positivos arbitrarios tales que $\sum w_{i}=1$.  La matriz

\begin{equation}
M(\xi)=\sum_{i=1}^n f(\mathrm{x}_{i})f(\mathrm{x}_{i})^{\top}\xi(\mathrm{x}_{i})\label{n=3}
\end{equation}
\\
se le llama la matriz de información del diseño aproximado.\\

Sea $\Xi$ el conjunto de todos los diseños aproximados y $\mathcal{M}$ el conjunto de matrices de informaci\'on que les corresponden:
\begin{equation}
	\mathcal{M} =\{M; M=M(\xi)\quad\text{para algunos $\xi\in\Xi$}\}    \nonumber
\end{equation}
\\
Sea $\Xi_{n}$ el conjunto de diseños aproximados, concentrado en $n$ puntos (con pesos distintos de cero).\\

Las propiedades b\'asicas de matrices de informaci\'on pueden enunciarse como un teorema.\\\\

\begin{theorem} Propiedades de las matrices de informaci\'on:
	\begin{itemize}
		\item[(i)] Cualquier matriz de informaci\'on es definida no negativa (en particular es sim\'etrica).\\
		
		Demostraci\'on:\\
		La demostraci\'on f\'acilmente se concluye de la definici\'on de matriz no negativa y del hecho de que para todo vector $z\in \mathbb{R}^{m}$ se cumple\\
				$$ z^{\top} M(\xi) z =\sum_{i=1}^n z^{\top}f(\mathrm{x}_{i})f(\mathrm{x}_{i})^{\top}z \xi(\mathrm{x}_{i})=\sum_{i=1}^n\| z^{\top}f(\mathrm{x}_{i})\|^{2} \xi(\mathrm{x}_{i})\geq0\label{n=3}$$
		\\
		\item[(ii)] Si $n<m$, siendo $m$ el n\'umero de par\'ametros, entonces $det M (\xi) = 0$.\\
		
		Demostraci\'on:\\
		Supongamos que $\xi$ tiene en su soporte $k < m$ puntos: $\mathrm{x}_{1},...,\mathrm{x}_{k}$. En el desarrollo del determinante aparecer\'an siempre al menos dos columnas iguales y por tanto el determinante ha de ser cero.\\
		
				
		\item[(iii)] El conjunto $\mathcal{M}$ es convexo.\\
		
		Demostraci\'on:\\
		Es necesario comprobar que para cualquier $\lambda \in [0,1]$ y para cualquier par de diseños $\xi_{1}$ y $\xi_{2}$, la matriz\
		$$ M=(1-\lambda)M(\xi_{1})+ \lambda M (\xi_{2})$$\
		pertenece al conjunto $\mathcal{M}$.\\
		
	    Definimos el diseño $\xi$ seg\'un la f\'ormula\\
		$$\xi=(1-\lambda)\xi_{1}+\lambda\xi_{2}$$\\
		mostraremos que $M=M(\xi)\in\mathcal{M}$; en efecto,\\
		\begin{equation}
		\begin{aligned}
		\nonumber
		M & = (1-\lambda)M(\xi_{1})+ \lambda M (\xi_{2}) \\
		\nonumber
		& = (1-\lambda) \sum_{i=1}^n f(\mathrm{x}_{i})f(\mathrm{x}_{i})^{\top}\xi_{1}(\mathrm{x}_{i}) + \lambda \sum_{i=1}^n f(\mathrm{x}_{i})f(\mathrm{x}_{i})^{\top}\xi_{2}(\mathrm{x}_{i}) \\
		\nonumber
		& = \sum_{i=1}^n f(\mathrm{x}_{i})f(\mathrm{x}_{i})^{\top} [ (1-\lambda)\xi_{1} (\mathrm{x}_{i}) + \lambda \xi_{2} (\mathrm{x}_{i})] \\
		\nonumber
		\nonumber
		& = M(\xi) \\
		\nonumber
		\end{aligned}
		\end{equation}
	 Con ello queda demostrado.\\
	\end{itemize}	
\end{theorem}


\section{Criterios de optimalidad}
Llamemos al diseño $\xi$ no singular si el $detM(\xi)\neq0$. Vamos a considerar s\'olo el caso de la estimaci\'on de todo el conjunto de par\'ametros. Aqu\'i, s\'olo los diseños no singulares son de inter\'es. El teorema de Gauss-Markov es válido para ello.\\

T\'ipicamente, no existe un diseño $\hat{\xi}$ tal que la matriz
$$M^{-1}(\hat{\xi})-M^{-1}(\xi),$$
es no negativa, donde $\xi$ es un diseño arbitrario. Por lo tanto, algunas funciones de matrices de informaci\'on, que tienen sentido estad\'istico, se utilizan como los criterios de optimalidad.\\

Consideremos algunos criterios de optimalidad de nuestro inter\'es.\\

\subsection{$D-criterio$}
El criterio de diseño m\'as usado en las aplicaciones es el de $D$-optimalidad, en el que la varianza generalizada de las estimaciones de los par\'ametros, o su logaritmo se reduce al m\'inimo.\\

El diseño $\xi=arg\ \underset{\xi}{max}\ |M(\xi)|=arg\ \underset{\xi}{min}\ |(M(\xi))^{-1}| $\ se llama diseño $D$-\'optimo.\\

El funciomal $$\Psi[M(\xi)]=
-Log |M(\xi)|$$ se llama criterio $D$-\'optimo; donde el funcional $\Psi$ definido en el espacio de elementos de la matriz de informaci\'on de $M\in\mathcal{M}$, se le denomina criterio de optimizaci\'on, ver Kiefer(1974).
\\

$D$-optimalidad, criterio del determinante, equivale a minimizar el volumen del elipsoide de variaci\'on de los estimadores lineales e insesgados de los par\'ametros desconocidos.\\
 


\subsection{$G-criterio$}
Para diseños continuos la varianza normalizada de la respuesta predicha es 
$$d(\mathrm{x},\xi)=f^{T}(\mathrm{x})M^{-1}(\xi)f(\mathrm{x})$$

El criterio $G$-\'optimo es de la forma
$$\underset{\mathrm{x}\in\mathcal X}{max}\ d(\mathrm{x},\xi)\rightarrow\underset{\xi}{inf}.$$
Tenga en cuenta que para el diseño discreto normado $\xi$,
$$d(\mathrm{x},\xi)=\dfrac{\sigma^{2}}{N}Cov(\boldsymbol{f}(\mathrm{x})^{\top}\boldsymbol{\theta});$$
es decir $d(\mathrm{x},\xi)$ es igual (a la constante de precisi\'on) a la varianza de un valor, predicha por el modelo en el punto $\mathrm{x}$.\\



\subsection{$D_{s}-criterio$}
Son apropiados cuando el inter\'es es estimar un subconjunto de $s$ par\'ametros de todo el $p-vector$ $\boldsymbol{\beta}$. Por lo tanto sin perdida de generalidad, los t\'erminos del modelo se pueden dividir en dos grupos\\
$$E(Y)=f^{T}(\mathrm{x})\boldsymbol{\beta}=f_{1}^{\top}(\mathrm{x})\boldsymbol{\beta}_{1}+f_{2}^{\top}\boldsymbol{\beta}_{2}$$\\
donde $\boldsymbol{\beta}_{1}\in\mathbb{R}^{s}$ y $\boldsymbol{\beta}_{2}\in\mathbb{R}^{p-s}$. Los $\boldsymbol{\beta}_{1}$ son los par\'ametros de inter\'es y los $p-s$ par\'ametros $\boldsymbol{\beta}_{2}$ suelen ser tratados como par\'ametros molestia.\\

Un ejemplo es cuando $\boldsymbol{\beta}_{1}$ corresponde a los factores experimentales y $\boldsymbol{\beta}_{2}$ corresponde a los par\'ametros de las variables de bloqueo (Ver ejemplos, cap\'itulo 15 en Optimum Experimental Designs, with SAS de A.C. Atkinson, A.N. Donev y R.D. Tobias). Un segundo ejemplo es cuando los experimentos est\'an diseñados para comprobar la bondad de ajuste de un modelo.\\

Para obtener expresiones para el criterio de diseño y funci\'on de varianza relacionadas, dividimos la matriz de informaci\'on como\\
$$M(\xi)=
\left(
\begin{array}{lr}
M_{11}(\xi) & M_{12}(\xi) \\
M_{12}^{T}(\xi) & M_{22}(\xi) \\
\end{array}
\right)
$$
\\

La matriz de covarianza para la estimaci\'on de m\'inimos cuadrados de $\beta_{1}$ es $M^{11}(\xi)$, la submatriz superior izquierda $s\times s$ de $M^{-1}(\xi)$. Se puede verificar, a partir de los resultados de la inversa de una matriz particionada (por ejemplo, Fedorov 1972, p. 24), que\
$$M^{11}(\xi)=\left\{M_{11}(\xi)-M_{12}(\xi)M_{22}^{-1}(\xi)M_{12}^{\top}(\xi)\right\}^{-1}$$\\
En consecuencia, el diseño $D_{s}$-\'optimo para $\beta_{1}$ maximiza el determinante\\
$$|M_{11}(\xi)-M_{12}(\xi)M_{22}^{-1}(\xi)M_{12}^{\top}(\xi)|=\frac{|M(\xi)|}{|M_{22}(\xi)|}.$$\\
Por otro lado, en la pr\'actica se pueden utilizar teoremas que proporcionan herramientas para la construcci\'on y el control de la optimizaci\'on de un diseño. Si la atenci\'on se limita al par\'ametro $\boldsymbol{\beta}$, se considera el conocido teorema general de equivalencia (cf. Kiefer y Wolfowitz(1990); cf. Silvey (1980); cf. Pulkelsheim(2006)).\\


\subsection{Teorema de equivalencia}
El siguiente resultado de Kiefer y Wolfowitz (1960) es de gran importancia en la teoría del diseño experimental óptimo.
\begin{theorem} (Kiefer-Wolfowitz. Teorema de equivalencia) Para el modelo (1.1), existe un diseño $D-$\'optimo bajo los supuestos clásicos de regresi\'on y las siguientes condiciones son equvalentes: 
	\begin{itemize}
		\item[(i)] $\xi^{*}$ es un diseño $D$-\'optimo.
		\item[(ii)] $\xi^{*}$ es un diseño $G$-\'optimo.
		\item[(iii)] $\underset{\mathrm{x}\in\mathcal X}{max}d(x,\xi^{*})=m.$
	\end{itemize}	
\end{theorem}

Por otra parte, todos los $D$-\'optimos diseños tienen la misma matriz de informaci\'on, y la función de predicci\'on de la varianza $d(x,\xi^{*})$ alcanza su m\'aximo en los puntos de cualquier diseño $D$-\'optimo con soporte finito.\\

Vale la pena subrayar que el teorema es cierto para los diseños que sean $D$-\'optimo en la clase de diseño aproximado.\\

Este teorema no s\'olo establece la equivalencia entre $D$ y $G$-criterios, sino que tambi\'en da la importante condici\'on necesaria y suficiente de $D$-optimalidad: Diseño $\xi^{*}$ es $D$-\'optimo si y s\'olo si $\underset{\mathrm{x}\in\mathcal X}{max}d(x,\xi^{*})=m.$\\

la demostraci\'on del teorema se puede encontrar en Kiefer y Wolfowitz $(1960)$. Muchos an\'alogos del teorema de Kiefer-Wolfowitz se pueden encontrar en Kiefer (1974), y aparece en forma m\'as general en Whittle (1973).\\

\section*{Ejemplo 3}
Sea $\eta(\mathrm{x}_{j},\boldsymbol{\theta})=\theta_{1}+\theta_{2}\mathrm{x}+\theta_{3}\mathrm{x}^{2}+\theta_{4}\mathrm{x}^{3}$; un modelo polinomial con  $\mathrm{x}\in[-1,1]$.\\

En el caso $D$-\'optimo, se verificar\'a a continuaci\'on que el diseño\\
$$\xi^{*}=
\begin{pmatrix}
-1 & -\sqrt{5}/5 & \sqrt{5}/5 & 1 \\
1/4 & 1/4 & 1/4 & 1/4 \\
\end{pmatrix}
$$
\\
es un diseño $D$-\'optimo.\\

En efecto, bastar\'a con mostrar que el diseño $\xi^{*}$ verifica las condiciones del criterio de $D$-\'optimilidad. Primero note que su matriz de momento es\\


\begin{equation}
\begin{aligned}
\nonumber
M(\xi^{*}) & = \sum_{\mathrm{x}\in \{-1,-\sqrt{5}/5,\sqrt{5}/5,1\} }[1,\mathrm{x},\mathrm{x}^{2},\mathrm{x}^{3}]^{\top}[1 \ \mathrm{x} \ \mathrm{x}^{2} \ \mathrm{x}^{3}](1/4)  \\\\
\nonumber
& = \begin{pmatrix}
1 & 0 & 3/5 & 0 \\
0 & 3/5 & 0 & 13/25 \\
3/5 & 0 & 13/25 & 0 \\
0 & 13/25 & 0 & 63/125 \\
\end{pmatrix} \\
\end{aligned}
\end{equation}
\\
Ahora\\
$$
M^{-1}(\xi^{*})=
\begin{pmatrix}
13/4 & 0 & -15/4 & 0 \\
0 & 63/4 & 0 & -65/4 \\
-15/4 & 0 & 25/4 & 0 \\
0 & 65/4 & 0 & 75/4 \\
\end{pmatrix} \\
$$
\\
Luego\\
\begin{equation}
\begin{aligned}
\nonumber
d(\mathrm{x},\xi^{*})&=f^{\top}(\mathrm{x})M^{-1}(\xi^{*})f(\mathrm{x})  \\\\
\nonumber
& = \frac{75}{4}\mathrm{x}^{6}-\dfrac{105}{4}\mathrm{x}^{4}+\frac{33}{4}\mathrm{x}^{2}+\frac{13}{4} \\
\end{aligned}
\end{equation}
\\


Por \'ultimo\\

m\'ax$d(\mathrm{x},\xi^{*})=4$ y dicha funci\'on tiene sus puntos cr\'iticos en los valores del diseño.\\

\begin{center}
	\includegraphics[width=10cm]{GRAFICA1}\\
	Figura 1.1 \ Ejemplo 3
\end{center}

Por lo tanto $\xi^{*}$ es $D$-\'optimo.
\\


\chapter{Diseños \'optimos en la presencia de efectos de bloques aleatorios}
Para un modelo lineal en la presencia de efectos de bloques aleatorios se describe la situaci\'on donde se tienen $b$ bloques, cada uno con $m_{i}$ observaciones. Por lo tanto, la $j$-\'esima observaci\'on $Y_{ij}$ al bloque $i$ se puede escribir como\\
\begin{equation}
Y_{ij} =\gamma_{i} + \beta_{0} + \mathbf{f}(\mathrm{x}_{ij})^{\top}\boldsymbol{\beta} + \epsilon_{ij}
\end{equation}
\\
donde $\mathrm{x}_{ij}$ son los puntos experimentales, $j=1,...,m_{i}$, $\boldsymbol{f}=(f_{1},...,f_{p})^{\top}$ es un conjunto de funciones (conocidas) de regresi\'on y $\beta_{0}\in\mathbb{R}$, $\boldsymbol{\beta}=(\beta_{1},...,\beta_{p})^{\top}$  son los par\'ametros desconocidos. $f$ puede ser la funci\'on identidad para regresi\'on lineal simple.\\

El término $\gamma_{i}$ es el efecto del $i$-\'esimo bloque aleatorio con $E(\gamma_{i})=0$ y $Var(\gamma_{i} )=\sigma_{\gamma}^{2}$. Los errores de observaci\'on aleatorio $\epsilon_{ij}$ se supone que son homoscedasticos, $E(\epsilon_{ij} )=0$, $Var(\epsilon_{ij})=\sigma^{2}$ y $Cov(\gamma_{i},\epsilon_{ij})=0$. El an\'alisis adicional depender\'a del cociente de varianza $d=\sigma_\gamma^{2} / \sigma^{2}$ . Nos centraremos en los par\'ametros de la poblaci\'on $\theta=(\beta_{0},\boldsymbol{\beta}^{\top})^{\top}$. Asumiremos que el n\'umero de observaciones por bloque es constante, es decir, $m_{i}=m$.\\

Denotemos por $\boldsymbol{Y}_{i}=(Y_{i1},...,Y_{im})^{\top}$  el vector de observaciones para el bloque $i$. La matriz de covarianza correspondiente $Cov(\boldsymbol{Y}_{i})=\sigma^{2}\boldsymbol{V}$ es completamente sim\'etrica, $\boldsymbol{V}=\boldsymbol{I}_{m}+d\boldsymbol{1}_{m}\boldsymbol{1}_{m}^{\top}$, donde $\boldsymbol{I}_{m}$ indica la matriz identidad $m \times m$ y $\boldsymbol{1}_{m}$ es un vector de longitud $m$ con todas las entradas iguales a uno. El efecto fijo individual de la matriz de diseño $\boldsymbol{X}_{i}=(\boldsymbol{1}_{m}\mid \boldsymbol{F}_{i})$ se puede descomponer en la primera columna de unos correspondiente a la intersección $\beta_{0}$ y la matriz de diseño para el vector de par\'ametros $\boldsymbol{\beta}$.\\ 

La inversa de $\boldsymbol{V}$ la podemos hallar mediante \'algebra matricial\
\begin{equation}
\begin{aligned}
\boldsymbol{V}^{-1} & = (\boldsymbol{I}_{m} + d\boldsymbol{1}_{m}\boldsymbol{1}_{m}^{\top})^{-1}\\
& = \boldsymbol{I} - d^{\frac{1}{2}}\boldsymbol{1}_{m}(\boldsymbol{I} + d\boldsymbol{1}_{m}^{\top}\boldsymbol{I}\boldsymbol{1}_{m})^{-1}d^{\frac{1}{2}}\boldsymbol{1}_{m}^{\top}\\
& = \boldsymbol{I} - d\boldsymbol{1}_{m}(\boldsymbol{I} + dm\boldsymbol{I})^{-1}\boldsymbol{1}_{m}^{\top}\\
& = \boldsymbol{I} - d\boldsymbol{1}_{m}(\boldsymbol{I}(1 + dm))^{-1}\boldsymbol{1}_{m}^{\top}\\
& = \boldsymbol{I} - \frac{d}{1 + dm}\boldsymbol{1}_{m}\boldsymbol{1}_{m}^{\top}
\end{aligned}
\end{equation}\\

 Luego, la matriz de informaci\'on por bloque, utilizando $(2\ldotp2)$ queda $\boldsymbol{X}_{i}^{\top}\boldsymbol{V}^{-1}\boldsymbol{X}_{i}=\boldsymbol{X}_{i}^{\top}\boldsymbol{X}_{i}-\frac{d}{1+md}\boldsymbol{X}_{i}^{\top}\boldsymbol{1}_{m}\boldsymbol{1}_{m}^{\top}\boldsymbol{X}_{i}$  que es proporcional a la inversa de la matriz de varianza-covarianza $Cov(\boldsymbol{\hat{\theta}}_{i})$ si $\boldsymbol{X}_{i}$ es de rango completo. As\'i $\boldsymbol{\theta}$ es estimado sobre una base por bloque
 
 \begin{equation}
 	\begin{aligned}
 	\nonumber
 		\boldsymbol{\hat{\theta}}_{i} & = (\boldsymbol{X}_{i}^{\top}\boldsymbol{V}^{-1}\boldsymbol{X}_{i})^{-1}\boldsymbol{X}_{i}^{\top}\boldsymbol{V}^{-1}\boldsymbol{Y}_{i}\\
 		\nonumber
  		& = (\boldsymbol{X}_{i}^{\top}\boldsymbol{X}_{i})^{-1}\boldsymbol{X}_{i}^{\top}\boldsymbol{Y}_{i}
 	\end{aligned}
 \end{equation}\\
 

 Sobre la base de la poblaci\'on el mejor estimador lineal insesgado se puede calcular como $\boldsymbol{\hat{\theta}}=\left(\displaystyle\sum_{i=1}^{b}\boldsymbol{X}_{i}^{\top}\boldsymbol{V}^{-1}\boldsymbol{X}_{i}\right)^{-1}\displaystyle\sum_{i=1}^{b}\boldsymbol{X}_{i}^{\top}\boldsymbol{V}^{-1}\boldsymbol{X}_{i}\boldsymbol{\hat{\theta}}_{i}$ si $d$ es conocido. Entonces $Cov(\boldsymbol{\hat{\theta}})=\sigma^{2}\boldsymbol{M}_{d}^{-1}$, donde $\boldsymbol{M}_{d}=\displaystyle\sum_{i=1}^{b}\boldsymbol{X}_{i}^{\top}\boldsymbol{V}^{-1}\boldsymbol{X}_{i}$ es la matriz de informaci\'on sobre la base de la poblaci\'on. El sub\'indice $d$ indica la dependencia del cociente de varianzas $d$. Como $\boldsymbol{M}_{d}=\displaystyle\sum_{i=1}^{b}\boldsymbol{X}_{i}^{\top}\boldsymbol{X}_{i}-\frac{d}{1+md}\displaystyle\sum_{i=1}^{b}\boldsymbol{X}_{i}^{\top}\boldsymbol{1}_{m}\boldsymbol{1}_{m}^{\top}\boldsymbol{X}_{i}$. \\\\
 
 La matriz de informaci\'on particionada de acuerdo a $\beta_{0}$ y $\boldsymbol{\beta}$, es
\\
\begin{equation}
\begin{aligned}
\nonumber
\boldsymbol{M}_{d} & = \displaystyle\sum_{i=1}^{b}\boldsymbol{X}_{i}^{\top}\boldsymbol{X}_{i}-\frac{d}{1+md}\displaystyle\sum_{i=1}^{b}\boldsymbol{X}_{i}^{\top}\boldsymbol{1}_{m}\boldsymbol{1}_{m}^{\top}\boldsymbol{X}_{i} \\\\
\nonumber
& = \displaystyle\sum_{i=1}^{b}(\boldsymbol{1}_{m}\mid \boldsymbol{F}_{i})^{\top}(\boldsymbol{1}_{m}\mid \boldsymbol{F}_{i})-\frac{d}{1+md}\displaystyle\sum_{i=1}^{b}(\boldsymbol{1}_{m}\mid \boldsymbol{F}_{i})^{\top}\boldsymbol{1}_{m}\boldsymbol{1}_{m}^{\top}(\boldsymbol{1}_{m}\mid \boldsymbol{F}_{i}) \\\\
\nonumber
& = \displaystyle\sum_{i=1}^{b}\begin{pmatrix}
\boldsymbol{1}_{m}^{\top} \\\\
\boldsymbol{F}_{i}^{\top} \\
\end{pmatrix}(\boldsymbol{1}_{m}\mid \boldsymbol{F}_{i})-\frac{d}{1+md}\displaystyle\sum_{i=1}^{b}\begin{pmatrix}
\boldsymbol{1}_{m}^{\top} \\\\
\boldsymbol{F}_{i}^{\top} \\
\end{pmatrix}\boldsymbol{1}_{m}\boldsymbol{1}_{m}^{\top}(\boldsymbol{1}_{m}\mid \boldsymbol{F}_{i}) \\\\
\nonumber
& = \displaystyle\sum_{i=1}^{b}\begin{pmatrix}
m & \boldsymbol{1}_{m}^{\top}\boldsymbol{F}_{i} \\\\
\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m} & \boldsymbol{F}_{i}^{\top}\boldsymbol{F}_{i} \\
\end{pmatrix}-\frac{d}{1+md}\displaystyle\sum_{i=1}^{b}\begin{pmatrix}
m\\\\
\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m} \\
\end{pmatrix}(m\mid \boldsymbol{1}_{m}^{\top}\boldsymbol{F}_{i}) \\\\
\nonumber
& = \displaystyle\sum_{i=1}^{b}\begin{pmatrix}
m & \boldsymbol{1}_{m}^{\top}\boldsymbol{F}_{i} \\\\
\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m} & \boldsymbol{F}_{i}^{\top}\boldsymbol{F}_{i} \\
\end{pmatrix}-\frac{d}{1+md}\displaystyle\sum_{i=1}^{b}\begin{pmatrix}
m^{2} & m\boldsymbol{1}_{m}^{\top}\boldsymbol{F}_{i} \\\\
\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m}m & \boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m}\boldsymbol{1}_{m}^{\top}\boldsymbol{F}_{i} \\
\end{pmatrix} \\\\
\nonumber
& = \frac{1}{1+md}\displaystyle\sum_{i=1}^{b}\begin{pmatrix}
m+m^{2}d & \boldsymbol{1}_{m}^{\top}\boldsymbol{F}_{i}+md\boldsymbol{1}_{m}^{\top}\boldsymbol{F}_{i} \\\\
\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m}+md\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m} & \boldsymbol{F}_{i}^{\top}\boldsymbol{F}_{i}+md\boldsymbol{F}_{i}^{\top}\boldsymbol{F}_{i} \\
\end{pmatrix} \\\\
\end{aligned}
\end{equation}
 $$-\frac{1}{1+md}\displaystyle\sum_{i=1}^{b}\begin{pmatrix}
 dm^{2} & dm\boldsymbol{1}_{m}^{\top}\boldsymbol{F}_{i} \\\\
 d\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m}m & d\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m}\boldsymbol{1}_{m}^{\top}\boldsymbol{F}_{i} \\
 \end{pmatrix}$$
 
 \begin{equation}
\boldsymbol{M}_{d}=\frac{1}{1+md}\left(\begin{tabular}{c | c}
$bm$ & $\displaystyle\sum_{i=1}^{b}\boldsymbol{1}_{m}^{T}\boldsymbol{F}_{i}$ \\
\hline
$\displaystyle\sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m}$ & $(1+md)\displaystyle\sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{F}_{i}-d\displaystyle\sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m}\boldsymbol{1}_{m}^{\top}\boldsymbol{F}_{i}$ \\
\end{tabular}\right)
 \end{equation} 
 \\
 \\
 
 Si el inter\'es est\'a en los efectos fijos $\boldsymbol{\beta}$ solamente, entonces las reglas para invertir las correspondientes matrices de informaci\'on parcial particionadas $\boldsymbol{M}_{\boldsymbol{\beta},d}^{-1}=Cov(\boldsymbol{\hat{\beta}})/\sigma^{2}$ es igual a\\
  \begin{equation}
  \boldsymbol{M}_{\boldsymbol{\beta},d}=\displaystyle\sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{F}_{i}-\frac{d}{1+md}\displaystyle\sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m}\boldsymbol{1}_{m}^{\top}\boldsymbol{F}_{i}-\frac{1}{bm} \ \frac{1}{1+md}\left(  \displaystyle\sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m}\right) \left( \displaystyle\sum_{i=1}^{b}\boldsymbol{1}_{m}^{\top}\boldsymbol{F}_{i}\right)  
 \end{equation} 
\\
\\
Tambi\'en consideramos los modelos l\'imites para $d=0$ y $d\rightarrow\infty$, respectivamente: Para $d=0$  obtenemos los modelos de efectos fijos y sin interceptos de bloques \\

\begin{equation}
Y_{ij}=\beta_{0}+\boldsymbol{f}(\mathrm{x}_{ij})^{\top}\boldsymbol{\beta}+\epsilon_{ij}
\end{equation}
\\
Obviamente, $\boldsymbol{M}_{d}$ tiende a $\boldsymbol{M}_{0}=\displaystyle\sum_{i=1}^{b}\boldsymbol{X}_{i}^{\top}\boldsymbol{X}_{i}$ para $d\rightarrow0$. Del mismo modo, $\boldsymbol{M}_{\boldsymbol{\beta},d}$ tiende a\\
\begin{equation}
\boldsymbol{M}_{\boldsymbol{\beta},0}=\displaystyle\sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{F}_{i}-\frac{1}{bm}\left( \displaystyle\sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m}\right) \left( \displaystyle\sum_{i=1}^{b}\boldsymbol{1}_{m}^{\top}\boldsymbol{F}_{i}\right) 
\end{equation}
\\
\\
Para $d\rightarrow\infty$ introducimos el modelo de efectos fijos con bloques fijos\\
\begin{equation}
Y_{ij}=\mu_{i}+\boldsymbol{f}(\mathrm{x}_{ij})^{\top}\boldsymbol{\beta}+\epsilon_{ij};   \  (\mu_{i}=\gamma_{i}+\beta_{0})
\end{equation}
\\
Aqu\'i, el vector de par\'ametros $(\mu_{1},...\mu_{b},\beta_{1},...,\beta_{p})^{\top}$ tiene dimensi\'on $b+p$ y la matriz de información correspondiente tiene la forma\\

\begin{equation}
\boldsymbol{M}_{\infty}=\left(\begin{tabular}{ccc|c}
 & & & $\boldsymbol{1}_{m}^{\top}\boldsymbol{F}_{1}$ \\
 & $m\boldsymbol{I}_{b}$ & & $\vdots$ \\
 & & & $\boldsymbol{1}_{m}^{\top}\boldsymbol{F}_{b}$ \\
\hline
$\boldsymbol{F}_{1}^{\top}$ & $\cdots$ & $\boldsymbol{F}_{b}^{\top}\boldsymbol{1}_{m}$ & $\displaystyle\sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{F}_{i}$\\
\end{tabular}\right)
\end{equation}
\\
\\
Para $\boldsymbol{\beta}$ la matriz de informaci\'on parcial correspondiente se puede calcular\\ 
\begin{equation}
\boldsymbol{M}_{\boldsymbol{\beta},\infty}=\displaystyle\sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{F}_{i}-\frac{1}{m}\displaystyle\sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m}\boldsymbol{1}_{m}^{\top}\boldsymbol{F}_{i}
\end{equation}
\\
De ahí se obtiene el siguiente resultado, que establece la matriz de informaci\'on parcial $\boldsymbol{M}_{\boldsymbol{\beta},d}$.\\
\\

$\mathbf{Lema 1.}$\\
\begin{equation}
\boldsymbol{M}_{\boldsymbol{\beta},d}=\frac{1}{1+md}\boldsymbol{M}_{\boldsymbol{\beta},0}+\frac{md}{1+md}\boldsymbol{M}_{\boldsymbol{\beta},\infty}
\end{equation}
\\
\textit{Tenga en cuenta que la matriz de informaci\'on parcial $\boldsymbol{M}_{\boldsymbol{\beta},d}$ tiende a $\boldsymbol{M}_{\boldsymbol{\beta},\infty}$ cuando $d$ tiende a  $\infty$.}\\
\\

\section{Aspectos del diseño}	
La calidad de los estimadores $\boldsymbol{\hat{\theta}}$ y $\boldsymbol{\hat{\beta}}$ depende de la configuraci\'on experimental $\mathrm{x}_{ij}$, $i=1,...,b$, $j=1,...,m$, a trav\'es de las matrices de informaci\'on  $\boldsymbol{M}_{d}$ y $\boldsymbol{M}_{\boldsymbol{\beta},d}$, respectivamente. El objetivo en el diseño experimental es elegir los puntos de una regi\'on diseño $\mathcal{X}$ con el fin de minimizar la covarianza $Cov(\boldsymbol{\hat{\theta}})$ o $Cov(\boldsymbol{\hat{\beta}})$ o partes de ella, lo cual es equivalente a maximizar las correspondientes matrices de información $\boldsymbol{M}_{d}$ o $\boldsymbol{M}_{\boldsymbol{\beta},d}$ respectivamente. Como esas matrices no est\'an completamente ordenadas, una optimizaci\'on uniforme no es posible, en general. Por lo tanto, algunos funcionales de valores reales que ponen \'enfasis en las propiedades particulares de los estimadores se optimizar\'an. El criterio de diseño m\'as usado es el $D-$criterio, que tiene como objetivo maximizar el determinante de la matriz de informaci\'on $\boldsymbol{M}_{d}$. Esto es equivalente a minimizar el volumen de un elipsoide de confianza para $\boldsymbol{\theta}$ bajo la suposici\'on de normalidad.\\

Si el inter\'es est\'a en los efectos $\boldsymbol{\beta}$ solamente, $D_{\boldsymbol{\beta}}-$optimalidad se define en t\'erminos de la determinante de la inversa $\boldsymbol{M}_{\boldsymbol{\beta},d}^{-1}$ de la correspondiente matriz de informaci\'on parcial. Como se ve a continuaci\'on,

\begin{equation}
\begin{aligned}
\nonumber
\det(\boldsymbol{M}_{d}) & =
\left|  \frac{1}{1+md}\left(\begin{tabular}{c | c}
$bm$ & $\displaystyle\sum_{i=1}^{b}\boldsymbol{1}_{m}^{T}\boldsymbol{F}_{i}$ \\
\hline
$\displaystyle\sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m}$ & $(1+md)\displaystyle\sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{F}_{i}-d\displaystyle\sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m}\boldsymbol{1}_{m}^{\top}\boldsymbol{F}_{i}$ \\
\end{tabular}\right)\right|
\end{aligned}
\end{equation}

$=\left( \frac{1}{1+md}\right)^{p+1}\left|bm\right||(1+md)\displaystyle\sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{F}_{i}-d\displaystyle\sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m}\boldsymbol{1}_{m}^{\top}\boldsymbol{F}_{i}$

$$-\displaystyle\sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m}(bm)^{-1}\displaystyle\sum_{i=1}^{b}\boldsymbol{1}_{m}^{T}\boldsymbol{F}_{i}|$$

$=\left( \frac{1}{1+md}\right)^{p+1}\left|bm\right||\displaystyle\sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{F}_{i}+md\displaystyle\sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{F}_{i}-d\displaystyle\sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m}\boldsymbol{1}_{m}^{\top}\boldsymbol{F}_{i}$

$$-\frac{1}{bm}\displaystyle\sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m}\displaystyle\sum_{i=1}^{b}\boldsymbol{1}_{m}^{T}\boldsymbol{F}_{i}|$$

$=\left( \frac{1}{1+md}\right)^{p+1}\left|bm\right||\displaystyle\sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{F}_{i}-\frac{1}{bm}\displaystyle\sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m}\displaystyle\sum_{i=1}^{b}\boldsymbol{1}_{m}^{T}\boldsymbol{F}_{i}$

$$+md\left( \displaystyle\sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{F}_{i}-\frac{1}{m}\displaystyle\sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m}\boldsymbol{1}_{m}^{\top}\boldsymbol{F}_{i}\right)|$$


$=\left( \frac{1}{1+md}\right)^{p+1}\left|bm\right|\left|\boldsymbol{M}_{\boldsymbol{\beta},0}+md\boldsymbol{M}_{\boldsymbol{\beta},\infty}\right|$\\

$=\left( \frac{1}{1+md}\right)^{p+1}\left|bm\right|\left|(1+md)\left(\frac{1}{1+md}\boldsymbol{M}_{\boldsymbol{\beta},0}+\frac{md}{1+md}\boldsymbol{M}_{\boldsymbol{\beta},\infty}\right)\right|$\\


$=\left( \frac{1}{1+md}\right)^{p+1}\left|bm\right|\left|(1+md)\boldsymbol{M}_{\boldsymbol{\beta},d}\right|$\\


$=
\left( \frac{1}{1+md}\right)^{p+1}\left|bm\right|(1+md)^{p}\left|\boldsymbol{M}_{\boldsymbol{\beta},d}\right|$\\

$=
\frac{bm}{1+md}\det\left( \boldsymbol{M}_{\boldsymbol{\beta},d}\right)$
\\

sujeta por la f\'ormula para el determinante de matrices particionadas. Por lo tanto, $D$ y $D_{\boldsymbol{\beta}}-$ optimalidad coinciden tambi\'en en modelos de interceptos aleatorios, un hecho bien conocido en el ajuste de efectos fijos.\\
\\

$\mathbf{Lema 2.}$ \textit{Un diseño $(\mathrm{x}_{ij})$ es $D-$\'optimo si y s\'olo si es $D_{\beta}$-\'optimo.}\\

Si tenemos en cuenta los diseños que son uniformes en todos los bloques, es decir, en que los par\'ametros experimentales son los mismos para cada bloque, $\mathrm{x}_{ij}\equiv \mathrm{x}_{j}$, , entonces la situaci\'on se simplifica radicalmente. En este caso las matrices de diseños individuales coinciden, $\boldsymbol{\mathbf{F}}_{i}=\boldsymbol{\mathbf{F}}_{1}$ y $\boldsymbol{\mathbf{X}}_{i}=\boldsymbol{\mathbf{X}}_{1}$, respectivamente, y $\boldsymbol{\mathbf{X}}_{1}$ tiene que ser de rango columna completa para permitir estimabilidad de $\boldsymbol{\theta}$. Adem\'as, $\boldsymbol{\hat{\theta}}=\frac{1}{b} \displaystyle\sum_{i=1}^b \boldsymbol{\hat{\theta}}_{i}$ se reduce a la media de los valores ajustados de forma individual para los par\'ametros.\\

La matriz de covarianza estandarizada $\boldsymbol{M}_{d}^{-1}$ se descompone de forma aditiva en la matriz correspondiente $\boldsymbol{M}_{0}^{-1}$ para el modelo de efectos fijos y sin intercepciones individuales y la variabilidad de la intersecci\'on aleatoria (v\'ease, por ejemplo, Entholzner y otros., (2005)). Para la matriz de informaci\'on reducida observamos\\

\begin{equation}
 \boldsymbol{M}_{\boldsymbol{\beta},0}=b\left(\boldsymbol{\mathbf{F}}_{1}^{\top}\boldsymbol{\mathbf{F}}_{1}-\frac{1}{m}\boldsymbol{\mathbf{F}}_{1}^{\top}\boldsymbol{1}_{m}\boldsymbol{1}_{m}^{\top}\boldsymbol{\mathbf{F}}_{1}\right)=\boldsymbol{\mathbf{M}}_{\boldsymbol{\beta},\infty}
\end{equation}  
\\

y, en consecuencia, por el Lema 1 $\boldsymbol{M}_{\boldsymbol{\beta},d}=\boldsymbol{M}_{\boldsymbol{\beta},0}$ es independiente de $d$. As\'i, el diseño $D-$\'optimo para el modelo de efectos fijos y sin intersecciones individuales es $D-$ y  $D_{\boldsymbol{\beta}}-$\'optimo para cada $d\geq0$ visto en el Lema 2.\\


\section*{Ejemplo de aplicación del modelo propuesto}

Consideremos el modelo de regresi\'on cuadr\'atico en dos variables sin interacciones\\

$
Y_{i} =\beta_{0} + \beta_{1}\mathrm{x}_{1i} + \beta_{2}\mathrm{x}_{2i} + \beta_{11}\mathrm{x}^{2}_{1i} + \beta_{22}\mathrm{x}^{2}_{2i} + \epsilon_{i};   \    (\mathrm{x}_{1i},\mathrm{x}_{2i})\in [-1,1]\times[-1,1].
$ 
\\

El diseño $\boldsymbol{\xi}$ el cual asigna iguales pesos $\frac{1}{9}$ a las cuatro esquinas $(\pm1,\pm1)$, a los cuatro puntos centrales de los lados $(0,\pm1)$; $(\pm1,0)$ y al punto central $(0,0)$ de la regi\'on experimental. Este diseño es $D$-\'optimo para este modelo. Si el diseño $\boldsymbol{\xi}$ es bloqueado como sigue\\

$\xi_{1}=
\begin{pmatrix}
(-1,0) & (0,1) & (1,-1) \\
1/3 & 1/3 & 1/3 \\
\end{pmatrix},$ \  $\xi_{2}=
\begin{pmatrix}
(-1,-1) & (0,0) & (1,1) \\
1/3 & 1/3 & 1/3 \\
\end{pmatrix},$ \\  $\xi_{3}=
\begin{pmatrix}
(-1,1) & (1,0) & (0,-1) \\
1/3 & 1/3 & 1/3 \\
\end{pmatrix}.$
\\\\

Entonces por el lema 2, \ $\boldsymbol{\xi}$ es $D$-\'optimal para el correspondiente modelo en la presencia de efectos de bloques con respuesta\\

$
Y_{ij} =\beta_{0} + \beta_{1}\mathrm{x}_{1ij} + \beta_{2}\mathrm{x}_{2ij} + \beta_{11}\mathrm{x}^{2}_{1ij} + \beta_{22}\mathrm{x}^{2}_{2ij} + \gamma_{i} + \epsilon_{ij},
$ 
\\

en la $j$-\'esima corrida sobre el bloque $i$, $(i=1,2,3; \ j=1,2,3)$ con pesos dados por $\boldsymbol{\xi}(i,(\mathrm{x}_{1ij},\mathrm{x}_{2ij}))=\frac{1}{3}\xi_{i}(\mathrm{x}_{1ij},\mathrm{x}_{2ij}).$ Adem\'as este diseño $D$-\'optimal por bloques no depende del cociente de varianza $d$.

\chapter{Conclusiones y trabajos futuros}
En el presente trabajo se desarrolla la matriz de informaci\'on de los criterios fijos en un modelo de regresi\'on lineal en la presencia de efectos de bloques aleatorios como una combinaci\'on convexa de las matrices de informaci\'on de los modelos l\'imites cuando la varianza del efecto de bloque es cero o tiende a infinito.\\

Por otro lado, se muestra que los diseños \'optimos para modelos de efectos fijos tambi\'en son \'optimos para modelos en la presencia de efectos de bloques aleatorios siempre que los bloques sean uniformes.\\


$D$ y $D_{\boldsymbol{\beta}}$-optimalidad coinciden tambi\'en en modelos en la presencia de efectos de bloques aleatorios, un hecho ya conocido en los escenarios con efectos fijos.\\

Para trabajos futuros se pueden considerar modelos en la presencia de efectos de bloques aleatorios donde los par\'ametros de regresi\'on interact\'uan con diferentes grupos o tratamientos. 

\renewcommand{\refname}{Bibliograf{\'\i}a}
\renewcommand{\chaptermark}[1]{\markboth {#1}{#1}}
\renewcommand{\sectionmark}[1]{\markright{#1}}
\addcontentsline{toc}{chapter}{Bibliograf\'{\i}a}
\bibliographystyle{./tex/aa}
\bibliography{aamnem99,aabib}

\begin{thebibliography}{11}
	\bibitem {AAC}
	{Atkins and Cheng.} {(1998).} {Optimal regression designs in the presenceof random block effects.}
	{JSPI 77, 321-335}.
	
	\bibitem{C}   
	{Cheng.}
	{Optimal regression designs under random blocks-efects models.} {(1995).} {Statistica Sinica 5, 485-497}.
	
	\bibitem{DAH}    
	{Debusho and Haines.} {(2007).} {V- and D-optimal population designs for the simple linear regression model with a random intercept term.}
	{Joumal of Statistical Planning and inference 138, 1116-1130}.
	
	\bibitem{EBSAS} 
	{Entholzner, M., Benda, N., Schwabe, R.} {(2005).} {A note on designs for estimating population parameters.}
	{Biometrical Letters 42, 25-41}.
	
	
	\bibitem{F}
	{Fedorov, V.V.} {(1972)} {Theory of Optimal Experiments.}
	{Academic Press, New York}.
	
	\bibitem{F}    
	{Fedorov, V.V.} {(1997).} {Model-Oriented Design of Experiments.}
	{Springer, New York}.
	
	\bibitem{GP}     
	{Goos, Peter.} {(2002)} {The Optimal Design of Blocked and Split-Plot.}
	{Springer, New York}.
	
	\bibitem{GAS}
	{GraBhoff, U and Schwabe, R.} {(2003).} {On the analysis of paired observations.}
	{Statistics and Probability Letters 65, 7-12}. 
	
	\bibitem {N}
	{Norell, L.} {(2006).} {Optimal designs for maximun likelihood estimators in the one-way random model. U.U.D.M.}
	{Report 2006:24. Department of Mathematics, Uppsala University}.
	
	\bibitem {SAS}
	{Schmelter, T. and Schwabe, R.} {(2008).} {On optimal designsin random intercept models.}
	{Tetra Mountains Mathematical Publications. 39, 145-53}.
	
	\bibitem {S}
	{Schwabe, R.} {(1996).} {Optimum Designs for Multi-Factor Models.}
	{Springer, New York}.
	
	\bibitem {S}
	{Silvey, S.D.} {(1980).} {Optimal Designs.}
	{Chapman \& Hall, London}.
	
	\bibitem {VCAB}
	{Van Breukelen, G.J.P., Candel, M.J.J.M. and Berger, M.P.F.} {(2008).}
	{Relative efficiency of enequal cluster sizes for variance component estimation in cluster randomized and multicentre trials.}
	{Statiscal Methods in Medical Research, 17, 439-58}.
	
	
\end{thebibliography}


\end{document}
