
\chapter{\textcolor{blue}{Diseños óptimos en la presencia de efectos de bloques
aleatorios}}

\label{AB2}

%recuperar numeracion arabica%\global\long\def\thechapter{\arabic{chapter}}



\lettrine[lines=4,loversize=-0.1,lraise=0.1,lhang=.2]{P}{}{ara un modelo lineal en la presencia de efectos de bloques aleatorios
se describe la situación donde se tienen $b$ bloques, cada uno con
$m_{i}$ observaciones. Por lo tanto,la $j$-ésima observación $Y_{ij}$
al bloque $i$ se puede escribir como\\
 
\begin{equation}
Y_{ij}=\gamma_{i}+\beta_{0}+\mathbf{f}(\mathrm{x}_{ij})^{\top}\boldsymbol{\beta}+\epsilon_{ij}
\end{equation}
\\
 donde $\mathrm{x}_{ij}$ son los puntos experimentales, $j=1,...,m_{i}$,
$\boldsymbol{f}=(f_{1},...,f_{p})^{\top}$ es un conjunto de funciones
(conocidas) de regresión y $\beta_{0}\in\mathbb{R}$, $\boldsymbol{\beta}=(\beta_{1},...,\beta_{p})^{\top}$
son los parámetros desconocidos. $f$ puede ser la función identidad
para regresión lineal simple.\\
}

El término $\gamma_{i}$ es el efecto del $i$-ésimo bloque aleatorio
con $E(\gamma_{i})=0$ y $Var(\gamma_{i})=\sigma_{\gamma}^{2}$. Los
errores de observación aleatorio $\epsilon_{ij}$ se supone que son
homoscedasticos, $E(\epsilon_{ij})=0$, $Var(\epsilon_{ij})=\sigma^{2}$
y $Cov(\gamma_{i},\epsilon_{ij})=0$. El análisis adicional dependerá
del cociente de varianza $d=\sigma_{\gamma}^{2}/\sigma^{2}$ . Nos
centraremos en los parámetros de la población $\theta=(\beta_{0},\boldsymbol{\beta}^{\top})^{\top}$.
Asumiremos que el número de observaciones por bloque es constante,
es decir, $m_{i}=m$.\\


Denotemos por $\boldsymbol{Y}_{i}=(Y_{i1},...,Y_{im})^{\top}$ el
vector de observaciones para el bloque $i$. La matriz de covarianza
correspondiente $Cov(\boldsymbol{Y}_{i})=\sigma^{2}\boldsymbol{V}$
es completamente simétrica, $\boldsymbol{V}=\boldsymbol{I}_{m}+d\boldsymbol{1}_{m}\boldsymbol{1}_{m}^{\top}$,
donde $\boldsymbol{I}_{m}$ indica la matriz identidad $m\times m$
y $\boldsymbol{1}_{m}$ es un vector de longitud $m$ con todas las
entradas iguales a uno. El efecto fijo individual de la matriz de
diseño $\boldsymbol{X}_{i}=(\boldsymbol{1}_{m}\mid\boldsymbol{F}_{i})$
se puede descomponer en la primera columna de unos correspondiente
a la intersección $\beta_{0}$ y la matriz de diseño para el vector
de parámetros $\boldsymbol{\beta}$.\\


La inversa de $\boldsymbol{V}$ la podemos hallar mediante álgebra
matricial\
\begin{equation}
\begin{aligned}\boldsymbol{V}^{-1} & =(\boldsymbol{I}_{m}+d\boldsymbol{1}_{m}\boldsymbol{1}_{m}^{\top})^{-1}\\
 & =\boldsymbol{I}-d^{\frac{1}{2}}\boldsymbol{1}_{m}(\boldsymbol{I}+d\boldsymbol{1}_{m}^{\top}\boldsymbol{I}\boldsymbol{1}_{m})^{-1}d^{\frac{1}{2}}\boldsymbol{1}_{m}^{\top}\\
 & =\boldsymbol{I}-d\boldsymbol{1}_{m}(\boldsymbol{I}+dm\boldsymbol{I})^{-1}\boldsymbol{1}_{m}^{\top}\\
 & =\boldsymbol{I}-d\boldsymbol{1}_{m}(\boldsymbol{I}(1+dm))^{-1}\boldsymbol{1}_{m}^{\top}\\
 & =\boldsymbol{I}-\frac{d}{1+dm}\boldsymbol{1}_{m}\boldsymbol{1}_{m}^{\top}
\end{aligned}
\end{equation}
\\


Luego, la matriz de información por bloque, utilizando $(2\ldotp2)$
queda $\boldsymbol{X}_{i}^{\top}\boldsymbol{V}^{-1}\boldsymbol{X}_{i}=\boldsymbol{X}_{i}^{\top}\boldsymbol{X}_{i}-\frac{d}{1+md}\boldsymbol{X}_{i}^{\top}\boldsymbol{1}_{m}\boldsymbol{1}_{m}^{\top}\boldsymbol{X}_{i}$
que es proporcional a la inversa de la matriz de varianza-covarianza
$Cov(\boldsymbol{\hat{\theta}}_{i})$ si $\boldsymbol{X}_{i}$ es
de rango completo. As\'{i} $\boldsymbol{\theta}$ es estimado sobre
una base por bloque

\begin{equation}
\begin{aligned}\boldsymbol{\hat{\theta}}_{i} & =(\boldsymbol{X}_{i}^{\top}\boldsymbol{V}^{-1}\boldsymbol{X}_{i})^{-1}\boldsymbol{X}_{i}^{\top}\boldsymbol{V}^{-1}\boldsymbol{Y}_{i}\\
 & =(\boldsymbol{X}_{i}^{\top}\boldsymbol{X}_{i})^{-1}\boldsymbol{X}_{i}^{\top}\boldsymbol{Y}_{i}
\end{aligned}
\end{equation}
\\


Sobre la base de la población el mejor estimador lineal insesgado
se puede calcular como $\boldsymbol{\hat{\theta}}=\left({\displaystyle \sum_{i=1}^{b}\boldsymbol{X}_{i}^{\top}\boldsymbol{V}^{-1}\boldsymbol{X}_{i}}\right)^{-1}{\displaystyle \sum_{i=1}^{b}\boldsymbol{X}_{i}^{\top}\boldsymbol{V}^{-1}\boldsymbol{X}_{i}\boldsymbol{\hat{\theta}}_{i}}$
si $d$ es conocido. Entonces $Cov(\boldsymbol{\hat{\theta}})=\sigma^{2}\boldsymbol{M}_{d}^{-1}$,
donde $\boldsymbol{M}_{d}={\displaystyle \sum_{i=1}^{b}\boldsymbol{X}_{i}^{\top}\boldsymbol{V}^{-1}\boldsymbol{X}_{i}}$
es la matriz de información sobre la base de la población. El sub\'{i}ndice
$d$ indica la dependencia del cociente de varianzas $d$. Como $\boldsymbol{M}_{d}={\displaystyle \sum_{i=1}^{b}\boldsymbol{X}_{i}^{\top}\boldsymbol{X}_{i}-\frac{d}{1+md}{\displaystyle \sum_{i=1}^{b}\boldsymbol{X}_{i}^{\top}\boldsymbol{1}_{m}\boldsymbol{1}_{m}^{\top}\boldsymbol{X}_{i}}}$.
\\
\\


La matriz de información particionada de acuerdo a $\beta_{0}$ y
$\boldsymbol{\beta}$, es \\
 
\begin{equation}
\begin{aligned}\boldsymbol{M}_{d} & ={\displaystyle \sum_{i=1}^{b}\boldsymbol{X}_{i}^{\top}\boldsymbol{X}_{i}-\frac{d}{1+md}{\displaystyle \sum_{i=1}^{b}\boldsymbol{X}_{i}^{\top}\boldsymbol{1}_{m}\boldsymbol{1}_{m}^{\top}\boldsymbol{X}_{i}}}\\
\\
 & ={\displaystyle \sum_{i=1}^{b}(\boldsymbol{1}_{m}\mid\boldsymbol{F}_{i})^{\top}(\boldsymbol{1}_{m}\mid\boldsymbol{F}_{i})-\frac{d}{1+md}{\displaystyle \sum_{i=1}^{b}(\boldsymbol{1}_{m}\mid\boldsymbol{F}_{i})^{\top}\boldsymbol{1}_{m}\boldsymbol{1}_{m}^{\top}(\boldsymbol{1}_{m}\mid\boldsymbol{F}_{i})}}\\
\\
 & ={\displaystyle \sum_{i=1}^{b}\begin{pmatrix}\boldsymbol{1}_{m}^{\top}\\
\\
\boldsymbol{F}_{i}^{\top}
\end{pmatrix}(\boldsymbol{1}_{m}\mid\boldsymbol{F}_{i})-\frac{d}{1+md}{\displaystyle \sum_{i=1}^{b}\begin{pmatrix}\boldsymbol{1}_{m}^{\top}\\
\\
\boldsymbol{F}_{i}^{\top}
\end{pmatrix}\boldsymbol{1}_{m}\boldsymbol{1}_{m}^{\top}(\boldsymbol{1}_{m}\mid\boldsymbol{F}_{i})}}\\
\\
 & ={\displaystyle \sum_{i=1}^{b}\begin{pmatrix}m & \boldsymbol{1}_{m}^{\top}\boldsymbol{F}_{i}\\
\\
\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m} & \boldsymbol{F}_{i}^{\top}\boldsymbol{F}_{i}
\end{pmatrix}-\frac{d}{1+md}{\displaystyle \sum_{i=1}^{b}\begin{pmatrix}m\\
\\
\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m}
\end{pmatrix}(m\mid\boldsymbol{1}_{m}^{\top}\boldsymbol{F}_{i})}}\\
\\
 & ={\displaystyle \sum_{i=1}^{b}\begin{pmatrix}m & \boldsymbol{1}_{m}^{\top}\boldsymbol{F}_{i}\\
\\
\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m} & \boldsymbol{F}_{i}^{\top}\boldsymbol{F}_{i}
\end{pmatrix}-\frac{d}{1+md}{\displaystyle \sum_{i=1}^{b}\begin{pmatrix}m^{2} & m\boldsymbol{1}_{m}^{\top}\boldsymbol{F}_{i}\\
\\
\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m}m & \boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m}\boldsymbol{1}_{m}^{\top}\boldsymbol{F}_{i}
\end{pmatrix}}}\\
\\
 & =\frac{1}{1+md}{\displaystyle \sum_{i=1}^{b}\begin{pmatrix}m+m^{2}d & \boldsymbol{1}_{m}^{\top}\boldsymbol{F}_{i}+md\boldsymbol{1}_{m}^{\top}\boldsymbol{F}_{i}\\
\\
\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m}+md\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m} & \boldsymbol{F}_{i}^{\top}\boldsymbol{F}_{i}+md\boldsymbol{F}_{i}^{\top}\boldsymbol{F}_{i}
\end{pmatrix}}\\
\\
\end{aligned}
\end{equation}
\[
-\frac{1}{1+md}{\displaystyle \sum_{i=1}^{b}\begin{pmatrix}dm^{2} & dm\boldsymbol{1}_{m}^{\top}\boldsymbol{F}_{i}\\
\\
d\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m}m & d\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m}\boldsymbol{1}_{m}^{\top}\boldsymbol{F}_{i}
\end{pmatrix}}
\]


\begin{equation}
\boldsymbol{M}_{d}=\frac{1}{1+md}\left(\begin{tabular}{c|c}
 \ensuremath{bm}  &  \ensuremath{{\displaystyle \sum_{i=1}^{b}\boldsymbol{1}_{m}^{T}\boldsymbol{F}_{i}}} \\
\hline \ensuremath{{\displaystyle \sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m}}}  &  \ensuremath{(1+md){\displaystyle \sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{F}_{i}-d{\displaystyle \sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m}\boldsymbol{1}_{m}^{\top}\boldsymbol{F}_{i}}}} 
\end{tabular}\right)
\end{equation}
\\
 \\


Si el interés está en los efectos fijos $\boldsymbol{\beta}$ solamente,
entonces las reglas para invertir las correspondientes matrices de
información parcial particionadas $\boldsymbol{M}_{\boldsymbol{\beta},d}^{-1}=Cov(\boldsymbol{\hat{\beta}})/\sigma^{2}$
es igual a\\
 
\begin{equation}
\boldsymbol{M}_{\boldsymbol{\beta},d}={\displaystyle \sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{F}_{i}-\frac{d}{1+md}{\displaystyle \sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m}\boldsymbol{1}_{m}^{\top}\boldsymbol{F}_{i}-\frac{1}{bm}\ \frac{1}{1+md}\left({\displaystyle \sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m}}\right)\left({\displaystyle \sum_{i=1}^{b}\boldsymbol{1}_{m}^{\top}\boldsymbol{F}_{i}}\right)}}
\end{equation}
\\
 \\
 También consideramos los modelos l\'{i}mites para $d=0$ y $d\rightarrow\infty$,
respectivamente: Para $d=0$ obtenemos los modelos de efectos fijos
y sin interceptos de bloques \\


\begin{equation}
Y_{ij}=\beta_{0}+\boldsymbol{f}(\mathrm{x}_{ij})^{\top}\boldsymbol{\beta}+\epsilon_{ij}
\end{equation}
\\
 Obviamente, $\boldsymbol{M}_{d}$ tiende a $\boldsymbol{M}_{0}={\displaystyle \sum_{i=1}^{b}\boldsymbol{X}_{i}^{\top}\boldsymbol{X}_{i}}$
para $d\rightarrow0$. Del mismo modo, $\boldsymbol{M}_{\boldsymbol{\beta},d}$
tiende a\\
 
\begin{equation}
\boldsymbol{M}_{\boldsymbol{\beta},0}={\displaystyle \sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{F}_{i}-\frac{1}{bm}\left({\displaystyle \sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m}}\right)\left({\displaystyle \sum_{i=1}^{b}\boldsymbol{1}_{m}^{\top}\boldsymbol{F}_{i}}\right)}
\end{equation}
\\
 \\
 Para $d\rightarrow\infty$ introducimos el modelo de efectos fijos
con bloques fijos\\
 
\begin{equation}
Y_{ij}=\mu_{i}+\boldsymbol{f}(\mathrm{x}_{ij})^{\top}\boldsymbol{\beta}+\epsilon_{ij};\ (\mu_{i}=\gamma_{i}+\beta_{0})
\end{equation}
\\
 Aqu\'{i}, el vector de parámetros $(\mu_{1},...\mu_{b},\beta_{1},...,\beta_{p})^{\top}$
tiene dimensión $b+p$ y la matriz de información correspondiente
tiene la forma\\


\begin{equation}
\boldsymbol{M}_{\infty}=\left(\begin{tabular}{ccc|c}
  &   &   &  \ensuremath{\boldsymbol{1}_{m}^{\top}\boldsymbol{F}_{1}} \\
 &  \ensuremath{m\boldsymbol{I}_{b}}  &   &  \ensuremath{\vdots} \\
 &   &   &  \ensuremath{\boldsymbol{1}_{m}^{\top}\boldsymbol{F}_{b}} \\
\hline \ensuremath{\boldsymbol{F}_{1}^{\top}}  &  \ensuremath{\cdots}  &  \ensuremath{\boldsymbol{F}_{b}^{\top}\boldsymbol{1}_{m}}  &  \ensuremath{{\displaystyle \sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{F}_{i}}} 
\end{tabular}\right)
\end{equation}
\\
 \\
 Para $\boldsymbol{\beta}$ la matriz de información parcial correspondiente
se puede calcular\\
 
\begin{equation}
\boldsymbol{M}_{\boldsymbol{\beta},\infty}={\displaystyle \sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{F}_{i}-\frac{1}{m}{\displaystyle \sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m}\boldsymbol{1}_{m}^{\top}\boldsymbol{F}_{i}}}
\end{equation}
\\
 De ahí se obtiene el siguiente resultado, que establece la matriz
de información parcial $\boldsymbol{M}_{\boldsymbol{\beta},d}$.\\
 \\


$\mathbf{Lema1.}$\\
 
\begin{equation}
\boldsymbol{M}_{\boldsymbol{\beta},d}=\frac{1}{1+md}\boldsymbol{M}_{\boldsymbol{\beta},0}+\frac{md}{1+md}\boldsymbol{M}_{\boldsymbol{\beta},\infty}
\end{equation}
\\
 \textit{Tenga en cuenta que la matriz de información parcial $\boldsymbol{M}_{\boldsymbol{\beta},d}$
tiende a $\boldsymbol{M}_{\boldsymbol{\beta},\infty}$ cuando $d$
tiende a $\infty$.}\\
 \\



\section{Aspectos del diseño}

La calidad de los estimadores $\boldsymbol{\hat{\theta}}$ y $\boldsymbol{\hat{\beta}}$
depende de la configuración experimental $\mathrm{x}_{ij}$, $i=1,...,b$,
$j=1,...,m$, a través de las matrices de información $\boldsymbol{M}_{d}$
y $\boldsymbol{M}_{\boldsymbol{\beta},d}$, respectivamente. El objetivo
en el diseño experimental es elegir los puntos de una región diseño
$\mathcal{X}$ con el fin de minimizar la covarianza $Cov(\boldsymbol{\hat{\theta}})$
o $Cov(\boldsymbol{\hat{\beta}})$ o partes de ella, lo cual es equivalente
a maximizar las correspondientes matrices de información $\boldsymbol{M}_{d}$
o $\boldsymbol{M}_{\boldsymbol{\beta},d}$ respectivamente. Como esas
matrices no están completamente ordenadas, una optimización uniforme
no es posible, en general. Por lo tanto, algunos funcionales de valores
reales que ponen énfasis en las propiedades particulares de los estimadores
se optimizarán. El criterio de diseño más usado es el $D-$criterio,
que tiene como objetivo maximizar el determinante de la matriz de
información $\boldsymbol{M}_{d}$. Esto es equivalente a minimizar
el volumen de un elipsoide de confianza para $\boldsymbol{\theta}$
bajo la suposición de normalidad.\\


Si el interés está en los efectos $\boldsymbol{\beta}$ solamente,
$D_{\boldsymbol{\beta}}-$optimalidad se define en términos de la
determinante de la inversa $\boldsymbol{M}_{\boldsymbol{\beta},d}^{-1}$
de la correspondiente matriz de información parcial. Como se ve a
continuación,

\begin{equation}
\begin{aligned}\det(\boldsymbol{M}_{d}) & =\left|\frac{1}{1+md}\left(\begin{tabular}{c|c}
 \ensuremath{bm}  &  \ensuremath{{\displaystyle \sum_{i=1}^{b}\boldsymbol{1}_{m}^{T}\boldsymbol{F}_{i}}} \\
\hline \ensuremath{{\displaystyle \sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m}}}  &  \ensuremath{(1+md){\displaystyle \sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{F}_{i}-d{\displaystyle \sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m}\boldsymbol{1}_{m}^{\top}\boldsymbol{F}_{i}}}} 
\end{tabular}\right)\right|\end{aligned}
\end{equation}


$=\left(\frac{1}{1+md}\right)^{p+1}\left|bm\right||(1+md){\displaystyle \sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{F}_{i}-d{\displaystyle \sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m}\boldsymbol{1}_{m}^{\top}\boldsymbol{F}_{i}}}$

\[
-{\displaystyle \sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m}(bm)^{-1}{\displaystyle \sum_{i=1}^{b}\boldsymbol{1}_{m}^{T}\boldsymbol{F}_{i}|}}
\]


$=\left(\frac{1}{1+md}\right)^{p+1}\left|bm\right||{\displaystyle \sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{F}_{i}+md{\displaystyle \sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{F}_{i}-d{\displaystyle \sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m}\boldsymbol{1}_{m}^{\top}\boldsymbol{F}_{i}}}}$

\[
-\frac{1}{bm}{\displaystyle \sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m}{\displaystyle \sum_{i=1}^{b}\boldsymbol{1}_{m}^{T}\boldsymbol{F}_{i}|}}
\]


$=\left(\frac{1}{1+md}\right)^{p+1}\left|bm\right||{\displaystyle \sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{F}_{i}-\frac{1}{bm}{\displaystyle \sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m}{\displaystyle \sum_{i=1}^{b}\boldsymbol{1}_{m}^{T}\boldsymbol{F}_{i}}}}$

\[
+md\left({\displaystyle \sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{F}_{i}-\frac{1}{m}{\displaystyle \sum_{i=1}^{b}\boldsymbol{F}_{i}^{\top}\boldsymbol{1}_{m}\boldsymbol{1}_{m}^{\top}\boldsymbol{F}_{i}}}\right)|
\]


$=\left(\frac{1}{1+md}\right)^{p+1}\left|bm\right|\left|\boldsymbol{M}_{\boldsymbol{\beta},0}+md\boldsymbol{M}_{\boldsymbol{\beta},\infty}\right|$\\


$=\left(\frac{1}{1+md}\right)^{p+1}\left|bm\right|\left|(1+md)\left(\frac{1}{1+md}\boldsymbol{M}_{\boldsymbol{\beta},0}+\frac{md}{1+md}\boldsymbol{M}_{\boldsymbol{\beta},\infty}\right)\right|$\\


$=\left(\frac{1}{1+md}\right)^{p+1}\left|bm\right|\left|(1+md)\boldsymbol{M}_{\boldsymbol{\beta},d}\right|$\\


$=\left(\frac{1}{1+md}\right)^{p+1}\left|bm\right|(1+md)^{p}\left|\boldsymbol{M}_{\boldsymbol{\beta},d}\right|$\\


$=\frac{bm}{1+md}\det\left(\boldsymbol{M}_{\boldsymbol{\beta},d}\right)$
\\


sujeta por la fórmula para el determinante de matrices particionadas.
Por lo tanto, $D$ y $D_{\boldsymbol{\beta}}-$ optimalidad coinciden
también en modelos de interceptos aleatorios, un hecho bien conocido
en el ajuste de efectos fijos.\\
 \\


$\mathbf{Lema2.}$ \textit{Un diseño $(\mathrm{x}_{ij})$ es $D-$óptimo
si y sólo si es $D_{\beta}$-óptimo.}\\


Si tenemos en cuenta los diseños que son uniformes en todos los bloques,
es decir, en que los parámetros experimentales son los mismos para
cada bloque, $\mathrm{x}_{ij}\equiv\mathrm{x}_{j}$, , entonces la
situación se simplifica radicalmente. En este caso las matrices de
diseños individuales coinciden, $\boldsymbol{\mathbf{F}}_{i}=\boldsymbol{\mathbf{F}}_{1}$
y $\boldsymbol{\mathbf{X}}_{i}=\boldsymbol{\mathbf{X}}_{1}$, respectivamente,
y $\boldsymbol{\mathbf{X}}_{1}$ tiene que ser de rango columna completa
para permitir estimabilidad de $\boldsymbol{\theta}$. Además, $\boldsymbol{\hat{\theta}}=\frac{1}{b}{\displaystyle \sum_{i=1}^{b}\boldsymbol{\hat{\theta}}_{i}}$
se reduce a la media de los valores ajustados de forma individual
para los parámetros.\\


La matriz de covarianza estandarizada $\boldsymbol{M}_{d}^{-1}$ se
descompone de forma aditiva en la matriz correspondiente $\boldsymbol{M}_{0}^{-1}$
para el modelo de efectos fijos y sin intercepciones individuales
y la variabilidad de la intersección aleatoria (véase, por ejemplo,
Entholzner y otros., (2005)). Para la matriz de información reducida
observamos\\


\begin{equation}
\boldsymbol{M}_{\boldsymbol{\beta},0}=b\left(\boldsymbol{\mathbf{F}}_{1}^{\top}\boldsymbol{\mathbf{F}}_{1}-\frac{1}{m}\boldsymbol{\mathbf{F}}_{1}^{\top}\boldsymbol{1}_{m}\boldsymbol{1}_{m}^{\top}\boldsymbol{\mathbf{F}}_{1}\right)=\boldsymbol{\mathbf{M}}_{\boldsymbol{\beta},\infty}
\end{equation}
\\


y, en consecuencia, por el Lema 1 $\boldsymbol{M}_{\boldsymbol{\beta},d}=\boldsymbol{M}_{\boldsymbol{\beta},0}$
es independiente de $d$. As\'{i}, el diseño $D-$óptimo para el modelo
de efectos fijos y sin intersecciones individuales es $D-$ y $D_{\boldsymbol{\beta}}-$óptimo
para cada $d\geq0$ visto en el Lema 2.\\


{Ejemplo de aplicación del modelo propuesto}

Consideremos el modelo de regresión cuadrático en dos variables sin
interacciones\\


$Y_{i}=\beta_{0}+\beta_{1}\mathrm{x}_{1i}+\beta_{2}\mathrm{x}_{2i}+\beta_{11}\mathrm{x}_{1i}^{2}+\beta_{22}\mathrm{x}_{2i}^{2}+\epsilon_{i};\ (\mathrm{x}_{1i},\mathrm{x}_{2i})\in[-1,1]\times[-1,1].$
\\


El diseño $\boldsymbol{\xi}$ el cual asigna iguales pesos $\frac{1}{9}$
a las cuatro esquinas $(\pm1,\pm1)$, a los cuatro puntos centrales
de los lados $(0,\pm1)$; $(\pm1,0)$ y al punto central $(0,0)$
de la región experimental. Este diseño es $D$-óptimo para este modelo.
Si el diseño $\boldsymbol{\xi}$ es bloqueado como sigue\\


$\xi_{1}=\begin{pmatrix}(-1,0) & (0,1) & (1,-1)\\
1/3 & 1/3 & 1/3
\end{pmatrix},$ \ $\xi_{2}=\begin{pmatrix}(-1,-1) & (0,0) & (1,1)\\
1/3 & 1/3 & 1/3
\end{pmatrix},$ \\
 $\xi_{3}=\begin{pmatrix}(-1,1) & (1,0) & (0,-1)\\
1/3 & 1/3 & 1/3
\end{pmatrix}.$ \\
\\


Entonces por el lema 2, \ $\boldsymbol{\xi}$ es $D$-óptimal para
el correspondiente modelo en la presencia de efectos de bloques con
respuesta\\


$Y_{ij}=\beta_{0}+\beta_{1}\mathrm{x}_{1ij}+\beta_{2}\mathrm{x}_{2ij}+\beta_{11}\mathrm{x}_{1ij}^{2}+\beta_{22}\mathrm{x}_{2ij}^{2}+\gamma_{i}+\epsilon_{ij},$
\\


en la $j$-ésima corrida sobre el bloque $i$, $(i=1,2,3;\ j=1,2,3)$
con pesos dados por $\boldsymbol{\xi}(i,(\mathrm{x}_{1ij},\mathrm{x}_{2ij}))=\frac{1}{3}\xi_{i}(\mathrm{x}_{1ij},\mathrm{x}_{2ij}).$
Además este diseño $D$-óptimal por bloques no depende del cociente
de varianza $d$. 
